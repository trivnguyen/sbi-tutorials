{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Model Validation and Calibration Test\n",
    "Credit: Tri Nguyen and Claude Code\n",
    "\n",
    "In this tutorial, we'll perform some validation tests on an NPE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Install sampling packages if needed\n",
    "try:\n",
    "    import tarp\n",
    "    print(\"tarp library is already installed.\")\n",
    "except ImportError:\n",
    "    # install tarp (which requires deprecation)\n",
    "    !pip install deprecation tarp\n",
    "    import tarp\n",
    "    print(\"tarp library installed successfully.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll quickly train a simple NPE model on Gaussian data. Then we'll perform calibration tests using (1) a simple rank-based calibration test and (2) a more advanced test using TARP [(Lemos et al. 2023)](https://arxiv.org/abs/2302.03026).\n",
    "\n",
    "We'll skip most of the steps for training NPE models, as they were covered in the previous tutotrials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian_simulator(\n",
    "    n_samples,\n",
    "    theta_mean=0.0,\n",
    "    theta_std=1.0,\n",
    "    a=1.0,\n",
    "    b=0.0,\n",
    "    noise_std=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample from Gaussian simulator: x = a*θ + b + ε\n",
    "\n",
    "    Prior: θ ~ N(theta_mean, theta_std^2)\n",
    "    Likelihood: x | θ ~ N(a*θ + b, noise_std^2)\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples: number of samples\n",
    "    - theta_mean: mean of prior p(θ)\n",
    "    - theta_std: std of prior p(θ)\n",
    "    - a: slope of linear relationship\n",
    "    - b: intercept\n",
    "    - noise_std: observational noise level\n",
    "\n",
    "    Returns:\n",
    "    - theta: (n_samples,) parameter values\n",
    "    - x: (n_samples,) observation values\n",
    "    \"\"\"\n",
    "    # Sample θ from prior\n",
    "    theta = np.random.normal(theta_mean, theta_std, size=n_samples)\n",
    "\n",
    "    # Sample x | θ = a*θ + b + ε\n",
    "    noise = np.random.normal(0, noise_std, size=n_samples)\n",
    "    x = a * theta + b + noise\n",
    "\n",
    "    return theta, x\n",
    "\n",
    "class GaussianPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that models p(θ|x) as a Gaussian.\n",
    "\n",
    "    Input: x (observation)\n",
    "    Output: mean μ(x) and std σ(x) for p(θ|x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),  # input dimension is 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Output layers for mean and log std of Gaussian\n",
    "        self.mean_layer = nn.Linear(hidden_size, 1)\n",
    "        self.logstd_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predict Gaussian parameters μ(x), σ(x)\"\"\"\n",
    "        features = self.network(x)\n",
    "        mean = self.mean_layer(features)\n",
    "        logstd = self.logstd_layer(features)\n",
    "        return mean, logstd\n",
    "\n",
    "    def log_prob(self, theta, x):\n",
    "        \"\"\"Compute log p(θ|x)\"\"\"\n",
    "        mean, logstd = self.forward(x)\n",
    "        var = torch.exp(2 * logstd)\n",
    "        log_prob = -0.5 * (np.log(2 * np.pi) + 2 * logstd + ((theta - mean) ** 2) / var)\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, x, n_samples=1000):\n",
    "        \"\"\"Sample θ ~ p(θ|x)\"\"\"\n",
    "        mean, logstd = self.forward(x)\n",
    "        mean = torch.repeat_interleave(mean.unsqueeze(-1), n_samples, dim=1)\n",
    "        logstd = torch.repeat_interleave(logstd.unsqueeze(-1), n_samples, dim=1)\n",
    "        samples = mean + torch.exp(logstd) * torch.randn(x.shape[0], n_samples, 1)\n",
    "        return samples\n",
    "\n",
    "def train_model(model, theta_train, x_train, epochs=50, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the posterior model.\n",
    "    \"\"\"\n",
    "    theta_tensor = torch.tensor(theta_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    x_tensor = torch.tensor(x_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, theta_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    print(f\"Training for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, theta_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            log_prob = model.log_prob(theta_batch, x_batch)\n",
    "            loss = -log_prob.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model on Gaussian data with a known noise level. During validation, we will experiment with both the correct noise level and an incorrect noise level to see how the calibration tests respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "n_training = 10_000\n",
    "a_true = 1.0\n",
    "b_true = 1.0\n",
    "noise_std = 1.0\n",
    "print(f\"Generating {n_training} training samples...\")\n",
    "theta_train, x_train = sample_gaussian_simulator(\n",
    "    n_training, a=a_true, b=b_true, noise_std=noise_std)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  θ shape: {theta_train.shape}\")\n",
    "print(f\"  x shape: {x_train.shape}\")\n",
    "\n",
    "gaussian_model = GaussianPosterior(hidden_size=64)\n",
    "gauss_losses = train_model(gaussian_model, theta_train, x_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we will perform some validation tests. First, we generate three sets of test datasets:\n",
    "\n",
    "1. Gaussian data with the same parameters as the training data (well-specified model).\n",
    "2. Gaussian data with a larger noise level.\n",
    "3. Gaussian data with a smaller noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a test dataset with different noise levels\n",
    "# feel free to play around with the noise levels, better yet, you can try changing a_true and b_true too!\n",
    "theta_test, x_test = sample_gaussian_simulator(\n",
    "    1000, a=a_true, b=b_true, noise_std=noise_std)\n",
    "theta_test_wide, x_test_wide = sample_gaussian_simulator(\n",
    "    1000, a=a_true, b=b_true, noise_std=noise_std*2)\n",
    "theta_test_narrow, x_test_narrow = sample_gaussian_simulator(\n",
    "    1000, a=a_true, b=b_true, noise_std=noise_std*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the example test dataset to visualize the effect of different noise levels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axes[0].hist(theta_test, bins=30, color='C0', histtype='step')\n",
    "axes[0].hist(theta_test_wide, bins=30, color='C1', histtype='step', linestyle='dashed')\n",
    "axes[0].hist(theta_test_narrow, bins=30, color='C2', histtype='step', linestyle='dotted')\n",
    "axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[0].set_ylabel('Count', fontsize=16)\n",
    "axes[0].set_title(r'Marginal $p(\\theta)$', fontsize=16)\n",
    "\n",
    "axes[1].hist(\n",
    "    x_test, bins=30, color='C0', histtype='step', label='Standard Noise')\n",
    "axes[1].hist(\n",
    "    x_test_wide, bins=30, color='C1', histtype='step', linestyle='dashed', label='Wide Noise')\n",
    "axes[1].hist(\n",
    "    x_test_narrow, bins=30, color='C2', histtype='step', linestyle='dotted', label='Narrow Noise')\n",
    "axes[1].set_xlabel(r'$x$', fontsize=16)\n",
    "axes[1].set_ylabel('Count', fontsize=16)\n",
    "axes[1].set_title(r'Marginal $p(x)$', fontsize=16)\n",
    "\n",
    "axes[1].legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_posteriors(model, x_values, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Sample from the posterior p(θ|x) for given x values.\n",
    "    \"\"\"\n",
    "    x_tensor = torch.tensor(x_values, dtype=torch.float32).reshape(-1, 1)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = model.sample(x_tensor, n_samples=n_samples)\n",
    "    return samples.numpy().squeeze()\n",
    "\n",
    "theta_samples = sample_posteriors(gaussian_model, x_test, n_samples=2000)\n",
    "theta_samples_wide = sample_posteriors(gaussian_model, x_test_wide, n_samples=2000)\n",
    "theta_samples_narrow = sample_posteriors(gaussian_model, x_test_narrow, n_samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Posterior Predictive Checks\n",
    "\n",
    "Before performing calibration tests, we'll first validate our model using **posterior predictive checks** (PPC). This is a crucial validation technique that checks whether the model can reproduce the observed data patterns.\n",
    "\n",
    "### What are Posterior Predictive Checks?\n",
    "\n",
    "Posterior predictive checks answer the question: *\"If my model is correct, can it generate data that looks like what I actually observed?\"*\n",
    "\n",
    "The process works as follows:\n",
    "1. For each observation $x_{\\text{obs}}$, sample parameters from the posterior: $\\theta \\sim p(\\theta | x_{\\text{obs}})$\n",
    "2. For each sampled $\\theta$, simulate new data: $x_{\\text{pred}} \\sim p(x | \\theta)$\n",
    "3. Compare the distribution of $x_{\\text{pred}}$ to $x_{\\text{obs}}$\n",
    "\n",
    "Mathematically, we're computing the **posterior predictive distribution**:\n",
    "$$p(x_{\\text{new}} | x_{\\text{obs}}) = \\int p(x_{\\text{new}} | \\theta) \\, p(\\theta | x_{\\text{obs}}) \\, d\\theta$$\n",
    "\n",
    "### Why is this useful?\n",
    "\n",
    "- **Detects model misspecification**: If the simulated data doesn't match the observed data, your model assumptions may be wrong\n",
    "- **Checks data generation process**: Validates that your model captures the true data-generating mechanism\n",
    "- **Complements calibration tests**: While calibration tests check if uncertainties are correct, PPCs check if the model structure is appropriate\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "- **Good fit**: Observed data falls within the distribution of predicted data\n",
    "- **Poor fit**: Observed data is systematically different from predicted data (e.g., different mean, variance, or shape)\n",
    "- **Model misspecification indicators**: \n",
    "  - Predicted data is consistently wider/narrower than observed\n",
    "  - Predicted data has different distributional shape\n",
    "  - Systematic biases in location or scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate posterior predictive samples for all three test datasets\n",
    "# note: since we re-run the true simulation, make sure to use the same a, b, noise_std as the original simulator\n",
    "print(\"Generating posterior predictive samples...\")\n",
    "x_pred = theta_samples * a_true + b_true + np.random.normal(0, noise_std, size=theta_samples.shape)\n",
    "x_pred_wide = theta_samples_wide * a_true + b_true + np.random.normal(0, noise_std, size=theta_samples_wide.shape)\n",
    "x_pred_narrow = theta_samples_narrow * a_true + b_true + np.random.normal(0, noise_std, size=theta_samples_narrow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior predictive checks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "datasets = [\n",
    "    (x_test, x_pred, \"Standard Noise\", \"C0\"),\n",
    "    (x_test_wide, x_pred_wide, \"Wide Noise\", \"C1\"),\n",
    "    (x_test_narrow, x_pred_narrow, \"Narrow Noise\", \"C2\")\n",
    "]\n",
    "\n",
    "for idx, (x_obs, x_pred, label, color) in enumerate(datasets):\n",
    "    # Left panel: Overlay of observed and predicted distributions\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Plot many predicted samples as light lines\n",
    "    for i in range(min(50, len(x_obs))):\n",
    "        ax.hist(x_pred[i], bins=30, alpha=0.05, color=color,\n",
    "                density=True, label='Predicted' if i == 0 else \"\")\n",
    "\n",
    "    # Plot observed data\n",
    "    ax.hist(x_obs, bins=30, alpha=0.7, color='black',\n",
    "            density=True, label='Observed', histtype='step', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel(r'$x$', fontsize=14)\n",
    "    ax.set_ylabel('Density', fontsize=14)\n",
    "    ax.set_title(f'{label}: Observed vs Predicted', fontsize=14)\n",
    "    ax.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This test is very sensitive to shifts in the true parameter values `a_true` and `b_true`. Change these values to see how the PPC results change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Calibration Tests\n",
    "\n",
    "When we train a model to predict posterior distributions $p(\\theta|x)$, it's crucial to verify that these predictions are **calibrated**. A calibrated model produces posterior distributions that accurately reflect the true uncertainty in our predictions.\n",
    "\n",
    "### What is Calibration?\n",
    "\n",
    "A well-calibrated posterior should satisfy:\n",
    "- If we draw a true parameter $\\theta^\\star$ from $p(\\theta|x)$ and claim \"$\\theta^\\star$ is in the 68% credible interval\", this should be true 68% of the time\n",
    "- More generally, for any credibility level $\\alpha$, the true parameter should fall within the $\\alpha$-credible region exactly $\\alpha$ fraction of the time\n",
    "\n",
    "If a model is **under-confident** (posteriors too wide), the true parameters will fall inside credible regions more often than expected.\n",
    "If a model is **over-confident** (posteriors too narrow), the true parameters will fall outside credible regions more often than expected.\n",
    "\n",
    "### Rank-Based Calibration Test\n",
    "\n",
    "The rank-based test is a simple but powerful way to check calibration. Here's how it works:\n",
    "\n",
    "1. **Generate test data**: Sample pairs $(\\theta^\\star, x^\\star)$ from the joint distribution $p(\\theta, x)$\n",
    "2. **Draw posterior samples**: For each $x^\\star$, draw N samples from the predicted posterior $p(\\theta|x^\\star)$\n",
    "3. **Compute ranks**: For each test case, count how many posterior samples are less than $\\theta^\\star$* (the \"rank\")\n",
    "4. **Check uniformity**: If the model is calibrated, these ranks should be uniformly distributed in $[0, N]$\n",
    "\n",
    "**Intuition**: Imagine you draw 100 samples from $p(\\theta|x^\\star)$). If the model is well-calibrated, the true value $\\theta^\\star$ should be equally likely to be the smallest, largest, or anywhere in between. If $\\theta^\\star$ consistently falls in the middle, the posterior is too wide (under-confident). If $\\theta^\\star$ is often at the extremes, the posterior is too narrow (over-confident).\n",
    "\n",
    "**Mathematical foundation**: For a well-calibrated model, the probability integral transform\n",
    "$$\\text{rank}(\\theta^*) = \\int_{-\\infty}^{\\theta^*} p(\\theta|x^*) d\\theta$$\n",
    "should follow a uniform distribution $U(0,1)$.\n",
    "\n",
    "In practice, instead of computing the integral, we approximate it using posterior samples:\n",
    "$$\\text{rank}(\\theta^*) \\approx \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}[\\theta_i < \\theta^*]$$\n",
    "where $\\theta_i$ are samples from $p(\\theta|x^*)$ and $\\mathbb{I}$ is the indicator function.\n",
    "\n",
    "Now let's implement this test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_based_calibration(theta_true, theta_samples):\n",
    "    \"\"\"\n",
    "    Compute ranks for the rank-based calibration test.\n",
    "\n",
    "    Parameters:\n",
    "    - theta_true: (n_tests,) array of true parameter values\n",
    "    - theta_samples: (n_tests, n_samples) array of posterior samples\n",
    "\n",
    "    Returns:\n",
    "    - ranks: (n_tests,) array of ranks (normalized to [0, 1])\n",
    "    \"\"\"\n",
    "    n_tests, n_samples = theta_samples.shape\n",
    "\n",
    "    # For each test case, count how many samples are less than the true value\n",
    "    ranks = np.sum(theta_samples < theta_true.reshape(-1, 1), axis=1)\n",
    "\n",
    "    # Normalize ranks to [0, 1]\n",
    "    normalized_ranks = ranks / n_samples\n",
    "\n",
    "    return normalized_ranks\n",
    "\n",
    "# Compute ranks for all three test datasets\n",
    "ranks_standard = rank_based_calibration(theta_test, theta_samples)\n",
    "ranks_wide = rank_based_calibration(theta_test_wide, theta_samples_wide)\n",
    "ranks_narrow = rank_based_calibration(theta_test_narrow, theta_samples_narrow)\n",
    "\n",
    "print(f\"Rank statistics for standard noise:\")\n",
    "print(f\"  Mean: {ranks_standard.mean():.3f} (expected: 0.500)\")\n",
    "print(f\"  Std:  {ranks_standard.std():.3f} (expected: {1/np.sqrt(12):.3f})\")\n",
    "print()\n",
    "print(f\"Rank statistics for wide noise (model should be over-confident):\")\n",
    "print(f\"  Mean: {ranks_wide.mean():.3f} (expected: 0.500)\")\n",
    "print(f\"  Std:  {ranks_wide.std():.3f} (expected: {1/np.sqrt(12):.3f})\")\n",
    "print()\n",
    "print(f\"Rank statistics for narrow noise (model should be under-confident):\")\n",
    "print(f\"  Mean: {ranks_narrow.mean():.3f} (expected: 0.500)\")\n",
    "print(f\"  Std:  {ranks_narrow.std():.3f} (expected: {1/np.sqrt(12):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histograms comparing all three\n",
    "ax = axes[0]\n",
    "ax.hist(ranks_standard, bins=30, color='C0', label='Standard Noise', density=True, histtype='step', linewidth=2)\n",
    "ax.hist(ranks_wide, bins=30, color='C1', label='Wide Noise', density=True, histtype='step', linewidth=2)\n",
    "ax.hist(ranks_narrow, bins=30, color='C2', label='Narrow Noise', density=True, histtype='step', linewidth=2)\n",
    "ax.axhline(1.0, color='black', linestyle='--', label='Uniform Distribution')\n",
    "ax.set_xlabel('Normalized Rank', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 2.5])\n",
    "\n",
    "ax = axes[1]\n",
    "# Standard noise\n",
    "sorted_ranks = np.sort(ranks_standard)\n",
    "theoretical_quantiles = np.linspace(0, 1, len(sorted_ranks))\n",
    "ax.plot(theoretical_quantiles, sorted_ranks, '-', lw=2, color='C0', label='Standard Noise')\n",
    "# Wide noise\n",
    "sorted_ranks_wide = np.sort(ranks_wide)\n",
    "ax.plot(theoretical_quantiles, sorted_ranks_wide, '-', lw=2, color='C1', label='Wide Noise')\n",
    "# Narrow noise\n",
    "sorted_ranks_narrow = np.sort(ranks_narrow)\n",
    "ax.plot(theoretical_quantiles, sorted_ranks_narrow, '-', lw=2, color='C2', label='Narrow Noise')\n",
    "\n",
    "# Calculate the perfect calibration + some uncertainty\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "\n",
    "# 1, 2, 3 sigma confidence bands\n",
    "# For uniform: standard error = 1/sqrt(12*n)\n",
    "# Using Gaussian approximation for empirical quantiles\n",
    "se = 1 / np.sqrt(12 * n)\n",
    "\n",
    "sigmas = [1, 2, 3]\n",
    "alphas = [0.4, 0.3, 0.2]\n",
    "quantiles = np.linspace(0, 1, 100)\n",
    "for sigma, alpha_val in zip(sigmas, alphas):\n",
    "    lower_band = np.maximum(quantiles - sigma * se, 0)\n",
    "    upper_band = np.minimum(quantiles + sigma * se, 1)\n",
    "\n",
    "    ax.fill_between(\n",
    "        quantiles, lower_band, upper_band,\n",
    "        alpha=alpha_val, color='gray',\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel('Predicted Quantiles', fontsize=12)\n",
    "ax.set_ylabel('Empirical Quantiles', fontsize=12)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform a Kolmogorov-Smirnov (KS) test to see if the rank distribution follows a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "ks_stat_standard, p_value_standard = kstest(ranks_standard, 'uniform')\n",
    "ks_stat_wide, p_value_wide = kstest(ranks_wide, 'uniform')\n",
    "ks_stat_narrow, p_value_narrow = kstest(ranks_narrow, 'uniform')\n",
    "\n",
    "print(f\"\"\"Kolmogorov-Smirnov Test Results\n",
    "(Tests if ranks are uniformly distributed)\n",
    "\n",
    "Standard Noise:\n",
    "  KS statistic: {ks_stat_standard:.4f}\n",
    "  p-value: {p_value_standard:.4f}\n",
    "  {'✓ Well calibrated' if p_value_standard > 0.05 else '✗ Poor calibration'}\n",
    "\n",
    "Wide Noise:\n",
    "  KS statistic: {ks_stat_wide:.4f}\n",
    "  p-value: {p_value_wide:.4f}\n",
    "  {'✓ Well calibrated' if p_value_wide > 0.05 else '✗ Poor calibration (over-confident)'}\n",
    "\n",
    "Narrow Noise:\n",
    "  KS statistic: {ks_stat_narrow:.4f}\n",
    "  p-value: {p_value_narrow:.4f}\n",
    "  {'✓ Well calibrated' if p_value_narrow > 0.05 else '✗ Poor calibration (under-confident)'}\n",
    "\n",
    "Note: p-value > 0.05 suggests good calibration\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TARP Calibration Test\n",
    "\n",
    "We will also perform the Tests of Accuracy with Random Points (TARP; [Lemos et al. 2023](https://arxiv.org/abs/2302.03026)). TARP is a sampling-based test specifically designed for high-dimensional posterior distributions and does not require explicit likelihood evaluation. With enough samples, [Lemos et al. 2023](https://arxiv.org/abs/2302.03026) demonstrates that TARP is both necessary and sufficient to test the accuracy of the posterior estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TARP\n",
    "ecp, alpha = tarp.get_tarp_coverage(\n",
    "    theta_samples.T.reshape(2000, -1, 1),  # samples need to have shape (n_samples, n_tests, dim)\n",
    "    theta_test.reshape(1000, 1), # true parameters need to have shape (n_tests, dim)\n",
    "    bootstrap=True\n",
    ")\n",
    "ecp_wide, alpha_wide = tarp.get_tarp_coverage(\n",
    "    theta_samples_wide.T.reshape(2000, -1, 1),\n",
    "    theta_test.reshape(1000, 1), # true parameters need to have shape (n_tests, dim)\n",
    "    bootstrap=True,\n",
    ")\n",
    "ecp_narrow, alpha_narrow = tarp.get_tarp_coverage(\n",
    "    theta_samples_narrow.T.reshape(2000, -1, 1),\n",
    "    theta_test.reshape(1000, 1), # true parameters need to have shape (n_tests, dim)\n",
    "    bootstrap=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "# Plot ECP curves and 1, 2, 3 sigma\n",
    "\n",
    "# plot the bootstrapping mean and perfect calibration line\n",
    "ax.plot(alpha, ecp.mean(axis=0), label='Standard Noise', color='C0', linewidth=2)\n",
    "ax.plot(alpha_wide, ecp_wide.mean(axis=0), label='Wide Noise', color='C1', linewidth=2)\n",
    "ax.plot(alpha_narrow, ecp_narrow.mean(axis=0), label='Narrow Noise', color='C2', linewidth=2)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "\n",
    "for k in [1, 2, 3]:\n",
    "    ax.fill_between(\n",
    "        alpha,\n",
    "        ecp.mean(axis=0) -  k * ecp.std(axis=0),\n",
    "        ecp.mean(axis=0) + k * ecp.std(axis=0),\n",
    "        color='C0', alpha=0.3)\n",
    "    ax.fill_between(\n",
    "        alpha_wide,\n",
    "        ecp_wide.mean(axis=0) -  k * ecp_wide.std(axis=0),\n",
    "        ecp_wide.mean(axis=0) + k * ecp_wide.std(axis=0),\n",
    "        color='C1', alpha=0.3)\n",
    "    ax.fill_between(\n",
    "        alpha_narrow,\n",
    "        ecp_narrow.mean(axis=0) -  k * ecp_narrow.std(axis=0),\n",
    "        ecp_narrow.mean(axis=0) + k * ecp_narrow.std(axis=0),\n",
    "        color='C2', alpha=0.3)\n",
    "\n",
    "\n",
    "# S-shaped regions for TARP\n",
    "x_region = np.linspace(0, 1, 100)\n",
    "\n",
    "# Overconfident: S-curve that's BELOW at low credibility, ABOVE at high credibility\n",
    "# Underconfident: S-curve that's ABOVE at low credibility, BELOW at high credibility\n",
    "overconfident_curve = x_region + 0.2 * np.sin(2 * np.pi * x_region)\n",
    "underconfident_curve = x_region - 0.2 * np.sin(2 * np.pi * x_region)\n",
    "\n",
    "ax.plot(x_region, overconfident_curve, color='k', ls='dotted', label='Overconfident')\n",
    "ax.plot(x_region, underconfident_curve, color='k', ls='dashdot', label='Underconfident')\n",
    "\n",
    "ax.set_xlabel('Credibility Level', fontsize=16)\n",
    "ax.set_ylabel('Expected Coverage', fontsize=16)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miscalibration manifests as S-shaped deviations from the diagonal: an **overconfident** posterior creates an S-curve that lies below the diagonal at low credibility levels and above at high levels, while an **underconfident** posterior shows the opposite pattern (above at low levels, below at high levels). \n",
    "\n",
    "**IMPORTANT**: TARP is a *binary* diagnostic - it tells you whether miscalibration exists but not the degree of miscalibration. The magnitude of deviation from the diagonal doesn't directly quantify how wrong your uncertainties are; it only indicates that the posterior is not properly calibrated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "Both rank-based and TARP tests successfully detect calibration issues:\n",
    "1. **Well-specified model** (standard noise): Both tests confirm good calibration\n",
    "2. **Over-confident model** (wide noise): Both detect that posteriors are too narrow\n",
    "3. **Under-confident model** (narrow noise): Both detect that posteriors are too wide\n",
    "\n",
    "The rank-based test is simpler to implement and interpret, but can only handle 1D or marginal posteriors. On the other hand, TARP can handle multivariate cases, but is tricker to interpret. Additionally, TARP does *NOT* tell you the degree to which the posterior is miscalibrated, only whether your posterior is calibrated or not. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
