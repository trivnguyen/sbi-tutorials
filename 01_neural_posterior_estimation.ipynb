{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Neural Posterior Estimation (NPE)\n",
    "Credit: Tri Nguyen and Claude Code\n",
    "\n",
    "In this tutorial, we'll implement a neural posterior estimation (NPE) algorithm from scratch using PyTorch. NPE is a simulation-based inference method that uses neural networks to approximate the posterior distribution of parameters given observed data.\n",
    "\n",
    "For simplicity, we'll use a one- or two-dimensional parameter space with a simple likelihood function. In each of the following experiments, our setup will be as follows:\n",
    "- Parameter: $\\theta$ sampled from a prior distribution $p(\\theta)$\n",
    "- Simulator: $p(x|\\theta)$ which implicitly defines the likelihood function\n",
    "- Observation: $x \\sim p(x|\\theta)$\n",
    "- True posterior: $p(\\theta|x)$\n",
    "\n",
    "**Goal**: Learn the true posterior $p(\\theta|x)$ using NPE, which approximates it with a neural network $q_\\phi(\\theta|x)$, where $\\phi$ are the parameters of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1a: Gaussian Posterior on Gaussian Data\n",
    "\n",
    "We'll start with a simple case where the true posterior distribution is Gaussian.\n",
    "We'll implement a simple NPE model where the distribution is approximated using a Gaussian distribution parameterized by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the training data\n",
    "\n",
    "Define a simulator that generates data from a 2D Gaussian likelihood with fixed mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian_joint(n_samples, mean=[0, 0], cov=[[1.0, 0.8], [0.8, 1.0]]):\n",
    "    \"\"\"\n",
    "    Sample from a 2D Gaussian joint distribution p(x, θ).\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples: number of samples\n",
    "    - mean: [mean_theta, mean_x]\n",
    "    - cov: 2x2 covariance matrix\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    theta = samples[:, 0]\n",
    "    x = samples[:, 1]\n",
    "    return theta, x\n",
    "\n",
    "# Visualize the Gaussian joint distribution\n",
    "theta_gauss_vis, x_gauss_vis = sample_gaussian_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_gauss_vis, x_gauss_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Gaussian Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_gauss_vis)} points from 2D Gaussian\")\n",
    "print(f\"Correlation between θ and x: {np.corrcoef(theta_gauss_vis, x_gauss_vis)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from Gaussian\n",
    "# feel free to experiment with different sample sizes, means, and covariances\n",
    "n_training = 10_000\n",
    "mean = [0, 0]\n",
    "cov = [[1.0, 0.8], [0.8, 1.0]]\n",
    "print(f\"Generating {n_training} training samples from Gaussian...\")\n",
    "\n",
    "theta_train_gauss, x_train_gauss = sample_gaussian_joint(n_training, mean=mean, cov=cov)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  θ shape: {theta_train_gauss.shape}\")\n",
    "print(f\"  x shape: {x_train_gauss.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(theta_train_gauss[:1000], x_train_gauss[:1000], alpha=0.4, s=10, color='purple')\n",
    "axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[0].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[0].set_title(r'Joint $p(x, \\theta)$', fontsize=16)\n",
    "\n",
    "axes[1].hist(theta_train_gauss, bins=50, alpha=0.7, color='C0')\n",
    "axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[1].set_ylabel('Count', fontsize=16)\n",
    "axes[1].set_title(r'Marginal $p(\\theta)$', fontsize=16)\n",
    "\n",
    "axes[2].hist(x_train_gauss, bins=50, alpha=0.7, color='C1')\n",
    "axes[2].set_xlabel(r'$x$', fontsize=16)\n",
    "axes[2].set_ylabel('Count', fontsize=16)\n",
    "axes[2].set_title(r'Marginal $p(x)$', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build an NPE with Gaussian Posterior\n",
    "\n",
    "We'll explicitly model the posterior as a Gaussian distribution:\n",
    "\n",
    "$$q_\\phi(\\theta|x) = \\mathcal{N}(\\theta; \\mu_\\phi(x), \\sigma^2_\\phi(x))$$\n",
    "\n",
    "The neural network learns the mean $\\mu_\\phi(x)$ and standard deviation $\\sigma_\\phi(x)$. Note that in practice, instead of learning $\\sigma_\\phi(x)$ directly, we often learn $\\log \\sigma_\\phi(x)$ to ensure numerical stability and positivity.\n",
    "\n",
    "To learn the model parameters $\\phi$, we minimize the negative log-likelihood of the observed data under the approximate posterior:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = -\\mathbb{E}_{p(x, \\theta)}[\\log q_\\phi(\\theta|x)] = -\\mathbb{E}_{p(x, \\theta)}\\left[-\\frac{1}{2}\\log(2\\pi\\sigma^2_\\phi(x)) - \\frac{(\\theta - \\mu_\\phi(x))^2}{2\\sigma^2_\\phi(x)}\\right]$$\n",
    "\n",
    "Under finite samples, this becomes:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = -\\frac{1}{N}\\sum_{i=1}^N \\log q_\\phi(\\theta_i|x_i)$$\n",
    "\n",
    "where $(x_i, \\theta_i) \\sim p(x, \\theta)$ are the observed data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that models p(θ|x) as a Gaussian.\n",
    "\n",
    "    Input: x (observation)\n",
    "    Output: mean μ(x) and std σ(x) for p(θ|x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),  # input dimension is 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Output layers for mean and log std of Gaussian\n",
    "        self.mean_layer = nn.Linear(hidden_size, 1)\n",
    "        self.logstd_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predict Gaussian parameters μ(x), σ(x)\"\"\"\n",
    "        features = self.network(x)\n",
    "        mean = self.mean_layer(features)\n",
    "        logstd = self.logstd_layer(features)\n",
    "        return mean, logstd\n",
    "\n",
    "    def log_prob(self, theta, x):\n",
    "        \"\"\"Compute log p(θ|x)\"\"\"\n",
    "        mean, logstd = self.forward(x)\n",
    "        var = torch.exp(2 * logstd)\n",
    "        log_prob = -0.5 * (np.log(2 * np.pi) + 2 * logstd + ((theta - mean) ** 2) / var)\n",
    "        return log_prob\n",
    "\n",
    "    def sample(self, x, n_samples=1000):\n",
    "        \"\"\"Sample θ ~ p(θ|x)\"\"\"\n",
    "        mean, logstd = self.forward(x)\n",
    "        samples = mean + torch.exp(logstd) * torch.randn(x.shape[0], n_samples, 1)\n",
    "        return samples\n",
    "\n",
    "gaussian_model_v1 = GaussianPosterior(hidden_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, theta_train, x_train, epochs=50, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the posterior model.\n",
    "    \"\"\"\n",
    "    theta_tensor = torch.tensor(theta_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    x_tensor = torch.tensor(x_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, theta_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    print(f\"Training for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, theta_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            log_prob = model.log_prob(theta_batch, x_batch)\n",
    "            loss = -log_prob.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    return losses\n",
    "\n",
    "# Train on Gaussian data\n",
    "# this is a simple model, so we only need a small number of epochs\n",
    "# feel free to adjust epochs, batch size, and learning rate\n",
    "gauss_losses_v1 = train_model(gaussian_model_v1, theta_train_gauss, x_train_gauss, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(gauss_losses_v1, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Negative Log-Likelihood', fontsize=16)\n",
    "plt.title('Training Loss (Gaussian Data)', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate: Gaussian Posterior on Gaussian Data\n",
    "\n",
    "Let's see if the learned $p(\\theta|x)$ matches the true posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_comparison(model, x_values, theta_train, x_train, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Compare learned p(θ|x) vs true p(θ|x) for different x values.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(x_values), figsize=(5*len(x_values), 5))\n",
    "    if len(x_values) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, x_val in enumerate(x_values):\n",
    "        x_tensor = torch.tensor([[x_val]], dtype=torch.float32)\n",
    "\n",
    "        # sample from the trained NPE model\n",
    "        with torch.no_grad():\n",
    "            mean, std = model(x_tensor)\n",
    "            samples = model.sample(x_tensor, n_samples=2000).squeeze().numpy()\n",
    "\n",
    "        # Plot the NPE samples\n",
    "        axes[idx].hist(\n",
    "            samples, bins=50, density=True, label='NPE', color='C0',\n",
    "            histtype='step', lw=2)\n",
    "\n",
    "        # Plot true posterior p(θ|x)\n",
    "        # since we do not have the true posterior in closed form, we will simply\n",
    "        # use the training data to estimate it locally around x_val\n",
    "        x_margin = 0.15\n",
    "        mask = np.abs(x_train - x_val) < x_margin\n",
    "        axes[idx].hist(theta_train[mask], bins=30, alpha=0.5, density=True,\n",
    "                    color='C1', label='True')\n",
    "\n",
    "        axes[idx].axvline(mean.item(), color='k', linestyle='--',\n",
    "                         lw=2, label=f'Mean')\n",
    "        axes[idx].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "        axes[idx].set_ylabel('Density', fontsize=16)\n",
    "        axes[idx].set_title(f'{title_prefix}$p(\\\\theta | x={x_val:.2f})$', fontsize=16)\n",
    "        axes[idx].legend(fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_posterior_comparison(\n",
    "    gaussian_model_v1, [-1.5, -0.5, 0.5, 1.5], theta_train_gauss, x_train_gauss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full posterior\n",
    "x_grid = np.linspace(x_train_gauss.min(), x_train_gauss.max(), 100)\n",
    "x_grid_tensor = torch.tensor(x_grid, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    means, stds = gaussian_model_v1(x_grid_tensor)\n",
    "    means = means.squeeze().numpy()\n",
    "    stds = stds.squeeze().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(theta_train_gauss[:1000], x_train_gauss[:1000], alpha=0.3, s=10,\n",
    "           label='Training data', color='k')\n",
    "ax.plot(means, x_grid, 'C0-', linewidth=3, label=r'Predicted Mean')\n",
    "ax.fill_betweenx(x_grid, means - stds, means + stds,\n",
    "                 alpha=0.3, label=r'Predicted $\\pm 1\\sigma$ uncertainty')\n",
    "ax.set_xlabel(r'Parameters $\\theta$', fontsize=16)\n",
    "ax.set_ylabel(r'Data $x$', fontsize=16)\n",
    "ax.set_title(r'Gaussian Model on Half-Moon: $p(\\theta|x)$', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1b: Gaussian Posterior on Half-Moon Data\n",
    "\n",
    "Now let's see what happens when we apply the **same Gaussian posterior** to a **non-Gaussian** distribution! We'll use a half-moon shaped $p(x, \\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate the training data\n",
    "\n",
    "The half-moon distribution has complex, non-linear structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_halfmoon_joint(n_samples, noise=0.1):\n",
    "    \"\"\"\n",
    "    Sample from half-moon joint distribution p(x, θ).\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples, labels = make_moons(n_samples=n_samples, noise=noise, random_state=None)\n",
    "\n",
    "    theta = samples[:, 0]  # First dimension\n",
    "    x = samples[:, 1]      # Second dimension\n",
    "\n",
    "    return theta, x\n",
    "\n",
    "# Visualize\n",
    "theta_moon_vis, x_moon_vis = sample_halfmoon_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_moon_vis, x_moon_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Half-Moon Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_moon_vis)} points from half-moon\")\n",
    "print(f\"θ range: [{theta_moon_vis.min():.2f}, {theta_moon_vis.max():.2f}]\")\n",
    "print(f\"x range: [{x_moon_vis.min():.2f}, {x_moon_vis.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training = 10000\n",
    "print(f\"Generating {n_training} training samples from half-moon...\")\n",
    "\n",
    "theta_train_moon, x_train_moon = sample_halfmoon_joint(n_training, noise=0.1)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  θ shape: {theta_train_gauss.shape}\")\n",
    "print(f\"  x shape: {x_train_gauss.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(theta_train_moon[:1000], x_train_moon[:1000], alpha=0.4, s=10, color='purple')\n",
    "axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[0].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[0].set_title(r'Joint $p(x, \\theta)$', fontsize=16)\n",
    "\n",
    "axes[1].hist(theta_train_moon, bins=50, alpha=0.7, color='C0')\n",
    "axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[1].set_ylabel('Count', fontsize=16)\n",
    "axes[1].set_title(r'Marginal $p(\\theta)$', fontsize=16)\n",
    "\n",
    "axes[2].hist(x_train_moon, bins=50, alpha=0.7, color='C1')\n",
    "axes[2].set_xlabel(r'$x$', fontsize=16)\n",
    "axes[2].set_ylabel('Count', fontsize=16)\n",
    "axes[2].set_title(r'Marginal $p(x)$', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Gaussian Posterior on Half-Moon Data\n",
    "\n",
    "Let's see what happens when we use a Gaussian posterior on non-Gaussian data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_model_v2 = GaussianPosterior(hidden_size=64)\n",
    "print(\"Training Gaussian model on half-moon data...\")\n",
    "\n",
    "gauss_losses_v2 = train_model(gaussian_model_v2, theta_train_moon, x_train_moon, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare losses\n",
    "fig, ax = plt.subplots(1, figsize=(6, 5))\n",
    "\n",
    "ax.plot(gauss_losses_v1, linewidth=2, label='Gaussian data')\n",
    "ax.plot(gauss_losses_v2, linewidth=2, label='Half-moon data')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Negative Log-Likelihood', fontsize=12)\n",
    "ax.set_title('Training Loss Comparison', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss (Gaussian data): {gauss_losses_v1[-1]:.4f}\")\n",
    "print(f\"Final loss (Half-moon data): {gauss_losses_v2[-1]:.4f}\")\n",
    "print(f\"\\nHigher loss on half-moon suggests model mismatch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate: Gaussian Posterior Fails on Half-Moon\n",
    "\n",
    "We will see that the Gaussian posterior fails to capture the true posterior shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_moon = [-0.5, 0.0, 0.5, 1.0]\n",
    "plot_posterior_comparison(\n",
    "    gaussian_model_v2, test_x_moon, theta_train_moon, x_train_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full posterior\n",
    "x_grid = np.linspace(x_train_moon.min(), x_train_moon.max(), 100)\n",
    "x_grid_tensor = torch.tensor(x_grid, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    means, stds = gaussian_model_v2(x_grid_tensor)\n",
    "    means = means.squeeze().numpy()\n",
    "    stds = stds.squeeze().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(theta_train_moon[:1000], x_train_moon[:1000], alpha=0.3, s=10,\n",
    "           label='Training data', color='k')\n",
    "ax.plot(means, x_grid, 'C0-', linewidth=3, label=r'Predicted Mean')\n",
    "ax.fill_betweenx(x_grid, means - stds, means + stds,\n",
    "                 alpha=0.3, label=r'Predicted $\\pm 1\\sigma$ uncertainty')\n",
    "ax.set_xlabel(r'Parameters $\\theta$', fontsize=16)\n",
    "ax.set_ylabel(r'Data $x$', fontsize=16)\n",
    "ax.set_title(r'Gaussian Model on Half-Moon: $p(\\theta|x)$', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Flow-based Posterior with Zuko\n",
    "\n",
    "Since the Gaussian posterior failed on the half-moon data, we need a more flexible posterior approximation. It is common to use normalizing flows for this purpose. \n",
    "\n",
    "Normalizing flows are a powerful class of generative models that transform a simple base distribution (like a Gaussian) into a complex target distribution through a series of invertible transformations. We'll use the `zuko` library with Neural Spline Flows (NSF).\n",
    "\n",
    "For more details on normalizing flows and the Zuko library, check out the [Zuko documentation](https://zuko.readthedocs.io/stable/tutorials.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install zuko if needed\n",
    "try:\n",
    "    import zuko\n",
    "    print(\"✓ zuko already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing zuko...\")\n",
    "    !pip install zuko\n",
    "    import zuko\n",
    "    print(\"✓ zuko installed\")\n",
    "\n",
    "from zuko.flows import NSF  # Neural Spline Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build the Flow-based NPE\n",
    "\n",
    "Normalizing flows provide a flexible way to model complex distributions by learning a transformation from a simple base distribution. Instead of directly modeling $p(\\theta|x)$ as a Gaussian, we learn a bijective (invertible) transformation:\n",
    "$$\\theta = T(z; x) \\quad \\text{where} \\quad z \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "Here:\n",
    "- $z$ is sampled from a simple Gaussian base distribution\n",
    "- $T(\\cdot; x)$ is a neural network that transforms $z$ into $\\theta$ conditioned on observation $x$\n",
    "- The transformation is **invertible** and **differentiable**, allowing us to compute densities via the change of variables formula:\n",
    "\n",
    "$$p(\\theta|x) = p(z) \\left| \\det \\frac{\\partial T^{-1}(\\theta; x)}{\\partial \\theta} \\right|$$\n",
    "\n",
    "Why flows work better than Gaussians:\n",
    "- Gaussians can only model unimodal, symmetric distributions\n",
    "- Flows can model arbitrary distributions by composing multiple transformations\n",
    "- Each transformation $T$ warps the space, allowing complex shapes like the half-moon\n",
    "\n",
    "We'll use **Neural Spline Flows (NSF)**, which use monotonic rational-quadratic splines as the transformation. This is a powerful and flexible choice for 1D posteriors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowPosterior(nn.Module):\n",
    "    \"\"\"\n",
    "    Flow-based model for p(θ|x) using Neural Spline Flows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64, embedding_size=8, transforms=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # It is common to embed the context x before passing to the flow\n",
    "        # in this case, it is not strictly necessary since x is 1D, but we include it for completeness\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),  # input dimension is 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embedding_size),\n",
    "        )\n",
    "\n",
    "        # NSF: Neural Spline Flow\n",
    "        self.flow = NSF(\n",
    "            features=1,                 # dimension of θ\n",
    "            context=embedding_size,     # dimension of embedded x\n",
    "            transforms=transforms,      # number of transformations\n",
    "            hidden_features=[hidden_size, hidden_size],  # hidden layers in each transform\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Return the flow conditioned on x\"\"\"\n",
    "        x_embedded = self.embedding_network(x)\n",
    "        return self.flow(x_embedded)\n",
    "\n",
    "    def log_prob(self, theta, x):\n",
    "        \"\"\"Compute log p(θ|x)\"\"\"\n",
    "        x_embedded = self.embedding_network(x)\n",
    "        return self.flow(x_embedded).log_prob(theta)\n",
    "\n",
    "    def sample(self, x, n_samples=1000):\n",
    "        \"\"\"Sample θ ~ p(θ|x)\"\"\"\n",
    "        x_embedded = self.embedding_network(x)\n",
    "        flow_i = self.flow(x_embedded)\n",
    "        return flow_i.sample((n_samples,))\n",
    "\n",
    "flow_model = FlowPosterior(\n",
    "    hidden_size=64,\n",
    "    embedding_size=8,\n",
    "    transforms=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Flow on Half-Moon Data\n",
    "\n",
    "The training objective is the same. We minimize the negative log-likelihood of the observed data under the flow-based posterior:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = -\\mathbb{E}_{p(x, \\theta)}[\\log q_\\phi(\\theta|x)]$$\n",
    "\n",
    "where $q_\\phi(\\theta|x)$ is now defined via the normalizing flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training flow model on half-moon data...\")\n",
    "flow_losses = train_model(flow_model, theta_train_moon, x_train_moon, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all losses\n",
    "fig, ax = plt.subplots(1, figsize=(6, 5))\n",
    "\n",
    "ax.plot(gauss_losses_v2, linewidth=2, label='Gaussian on half-moon')\n",
    "ax.plot(flow_losses, linewidth=2, label='Flow on half-moon')\n",
    "ax.set_xlabel('Epoch', fontsize=16)\n",
    "ax.set_ylabel('Negative Log-Likelihood', fontsize=16)\n",
    "ax.set_title('Training Loss: Gaussian vs Flow', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss - Gaussian: {gauss_losses_v2[-1]:.4f}\")\n",
    "print(f\"Final loss - Flow: {flow_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {gauss_losses_v2[-1] - flow_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate Flow Model\n",
    "\n",
    "The flow should capture the complex posterior much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_flow(model, x_values, theta_train, x_train):\n",
    "    \"\"\"\n",
    "    Visualize p(θ|x) from flow model.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(x_values), figsize=(5*len(x_values), 5))\n",
    "    if len(x_values) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, x_val in enumerate(x_values):\n",
    "        x_tensor = torch.FloatTensor([[x_val]])\n",
    "        with torch.no_grad():\n",
    "            samples = model.sample(x_tensor, n_samples=2000).squeeze().numpy()\n",
    "\n",
    "        axes[idx].hist(\n",
    "            samples, bins=50, density=True, label='NPE (Flow)', color='C0',\n",
    "            histtype='step', lw=2)\n",
    "\n",
    "        # True conditional\n",
    "        x_margin = 0.1\n",
    "        mask = np.abs(x_train - x_val) < x_margin\n",
    "        axes[idx].hist(\n",
    "            theta_train[mask], bins=30, alpha=0.5, density=True, color='C1', label='True')\n",
    "\n",
    "        axes[idx].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "        axes[idx].set_ylabel('Density', fontsize=16)\n",
    "        axes[idx].set_title(f'$p(\\\\theta | x={x_val:.2f})$', fontsize=16)\n",
    "        axes[idx].legend(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_posterior_flow(flow_model, test_x_moon, theta_train_moon, x_train_moon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize full posterior from flow model as a contour plot\n",
    "print(\"Computing posterior density on grid...\")\n",
    "\n",
    "# Create a 2D grid\n",
    "theta_grid = np.linspace(theta_train_moon.min(), theta_train_moon.max(), 100)\n",
    "x_grid = np.linspace(x_train_moon.min(), x_train_moon.max(), 100)\n",
    "\n",
    "theta_mesh, x_mesh = np.meshgrid(theta_grid, x_grid)\n",
    "\n",
    "# Compute log p(theta|x) on the grid\n",
    "log_probs = np.zeros_like(theta_mesh)\n",
    "for i in range(len(x_grid)):\n",
    "    x_val = x_grid[i]\n",
    "    x_tensor = torch.tensor([[x_val]], dtype=torch.float32).repeat(len(theta_grid), 1)\n",
    "    theta_tensor = torch.tensor(theta_grid, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_probs[i, :] = flow_model.log_prob(theta_tensor, x_tensor).squeeze().numpy()\n",
    "\n",
    "# Convert to probability density (exponentiate)\n",
    "probs = np.exp(log_probs)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 6))\n",
    "\n",
    "# Plot contour of the learned posterior\n",
    "contour = ax.contourf(theta_mesh, x_mesh, probs, levels=10, cmap='viridis', alpha=0.8)\n",
    "ax.contour(theta_mesh, x_mesh, probs, levels=10, colors='k', linewidths=0.5, alpha=0.5)\n",
    "\n",
    "# Overlay training data\n",
    "ax.scatter(theta_train_moon[:1000], x_train_moon[:1000], alpha=1, s=10,\n",
    "           label='Training data', color='k')\n",
    "\n",
    "ax.set_xlabel(r'Parameters $\\theta$', fontsize=16)\n",
    "ax.set_ylabel(r'Data $x$', fontsize=16)\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r'Posterior $q_\\theta(\\theta|x)$', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we explored two approaches to neural posterior estimation and demonstrated why normalizing flows have become the standard method.\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**Part 1a: Gaussian Posterior on Gaussian Data**\n",
    "- Modeled $p(\\theta|x) = \\mathcal{N}(\\theta; \\mu(x), \\sigma^2(x))$ using a simple neural network\n",
    "- Worked well because the true posterior was also Gaussian\n",
    "- Fast training and easy implementation\n",
    "\n",
    "**Part 1b: Gaussian Posterior on Half-Moon Data**\n",
    "- Applied the same Gaussian model to non-Gaussian data\n",
    "- **Failed** to capture the complex curved structure\n",
    "- Higher training loss and poor posterior approximations\n",
    "- Demonstrated the limitation: Gaussian posteriors cannot represent arbitrary distributions\n",
    "\n",
    "**Part 2: Flow-based Posterior**\n",
    "- Used normalizing flows (Neural Spline Flows) to model $p(\\theta|x)$\n",
    "- Successfully captured the half-moon structure\n",
    "- Significantly lower training loss (0.47 vs 0.97)\n",
    "- Accurate posterior approximations across all conditioning values\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**For NPE applications, use normalizing flows.** \n",
    "\n",
    "While Gaussian posteriors are simpler, they are only suitable when you *know* the posterior is approximately Gaussian (which is rarely the case in practice). Flows provide:\n",
    "- **Flexibility**: Can represent arbitrary posterior distributions\n",
    "- **Reliability**: Work well even when posterior structure is unknown\n",
    "- **Standard practice**: The de facto choice in modern simulation-based inference\n",
    "\n",
    "The additional complexity of flows over Gaussian models is minimal with modern libraries like `zuko`, but the benefits are substantial. Unless you have strong prior knowledge that your posterior is Gaussian, just use flows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
