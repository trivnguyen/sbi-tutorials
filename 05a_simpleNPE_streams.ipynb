{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5a: Neural Posterior Estimation to infer parameters of an impacting subhalo from stellar streams\n",
    "Credit: Tri Nguyen and Claude Code\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we will demonstrate how to implement a Neural Posterior Estimation (NPE) model to infer the parameters of a dark matter subhalo that has impacted a stellar stream. This is a realistic astrophysics application of simulation-based inference (SBI), where we want to understand subhalo properties from observable stream morphology.\n",
    "\n",
    "**Physical Context**: When a dark matter subhalo passes near a stellar stream, it perturbs the positions and velocities of the stream stars. The pattern of this perturbation encodes information about the subhalo's mass and velocity. By training an NPE model on simulated stream-subhalo encounters, we can infer the subhalo properties from observations.\n",
    "\n",
    "**What we'll cover:**\n",
    "- Loading and visualizing simulated stellar stream data with subhalo impacts\n",
    "- Preparing training data for NPE with proper normalization\n",
    "- Training a simple NPE model (MLP embedding + Neural Spline Flow)\n",
    "- Validating the model with coverage tests (rank-based and TARP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Simulation Data\n",
    "\n",
    "We will first load the stream data from the `data` directory. The data contains 10,000 simulated stellar streams, each with a different subhalo impact scenario. The streams are generated using the `StreamSculptor` package [Nibauer et al. (2024)](https://arxiv.org/abs/2410.21174v1).\n",
    "\n",
    "**Data Structure:**\n",
    "- `params_dict`: A dictionary containing the simulation parameters for each stream:\n",
    "  - `M_sh`: Subhalo mass in solar masses ($\\mathrm{M_\\odot}$). Shape: `(10000,)`\n",
    "  - `vz_impact`: Impact velocity in the $z$ direction (line-of-sight) in km/s. Shape: `(10000,)`\n",
    "- `streams`: The 6D phase-space coordinates of stream stars: $(x, y, z, v_x, v_y, v_z)$\n",
    "  - Shape: `(10000, N_stars, 6)` where `N_stars = 9998` for all streams in this dataset\n",
    "  \n",
    "**Physical Interpretation**: Each stream represents a tidal disruption event that has been perturbed by a passing subhalo. The subhalo mass determines the strength of the perturbation, while the impact velocity affects the timescale and spatial pattern of the disturbance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "params_dict = data['params_dict']\n",
    "streams = data['streams']  # shape: (N_samples, N_particles, 6) # 6 corresponds to (x, y, z, vx, vy, vz)\n",
    "\n",
    "# params_dict contain two columns: M_sh (the subhalo mass) and vz_impact (the impact velocity along the line of sight)\n",
    "M_sh = params_dict['M_sh']\n",
    "vz_impact = params_dict['vz_impact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Plot a few example streams to visualize the data distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(streams[i, :, 1], streams[i, :, 2], 'k.', markersize=1, alpha=0.5)\n",
    "    ax.set_title(f'Stream Example {i+1}', fontsize=16)\n",
    "    ax.set_xlabel('y [kpc]', fontsize=16)\n",
    "    ax.set_ylabel('z [kpc]', fontsize=16)\n",
    "\n",
    "    # print some info about the stream\n",
    "    text = f'M_sh: {M_sh[i]:.2e} Msun\\nvz_impact: {vz_impact[i]:.2f} km/s'\n",
    "    ax.text(0.95, 0.05, text, transform=ax.transAxes, fontsize=12,\n",
    "            va='bottom', ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Plot the distribution of the two parameters to visualize the prior distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(np.log10(M_sh), bins=30, color='C0', alpha=0.7)\n",
    "axes[0].set_xlabel(r'$\\log_{10}(M_\\mathrm{sh}/\\mathrm{M}_{\\odot})$', fontsize=16)\n",
    "axes[0].set_ylabel('Frequency', fontsize=16)\n",
    "\n",
    "axes[1].hist(vz_impact, bins=30, color='C1', alpha=0.7)\n",
    "axes[1].set_xlabel('$v_{z}$ [km/s]', fontsize=16)\n",
    "axes[1].set_ylabel('Frequency', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Distributions\n",
    "\n",
    "From the histograms above, we can see that the parameters are sampled from the following distributions:\n",
    "\n",
    "- $\\log_{10} (M_\\mathrm{sh} / \\mathrm{M_\\odot}) \\sim \\mathcal{U}(6, 8)$\n",
    "- $v_{z} \\sim \\mathcal{U}(-50, 50) \\, \\mathrm{km/s}$\n",
    "\n",
    "where $\\mathcal{U}(a, b)$ denotes a uniform distribution between $a$ and $b$. \n",
    "\n",
    "**IMPORTANT**: In NPE, the distributions of parameters in the training dataset effectively define the **prior distributions** $p(\\theta)$. The trained model will learn the posterior $p(\\theta|x)$ based on these priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NPE: MLP embedding + NSF\n",
    "\n",
    "We will implement a simple NPE model with a Multi-Layer Perceptron (MLP) embedding network and a Neural Spline Flow (NSF) density estimator. In the subsequent tutorial, we will explore more advanced architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the Data\n",
    "\n",
    "We first prepare the data for the SimpleNPE model. The MLP embedding network takes input of shape `(N_batch, N_features)`, so we need to flatten the stream data from shape `(N_samples, N_stars, 6)` to `(N_samples, N_stars * 6)`.\n",
    "\n",
    "**Key preprocessing steps:**\n",
    "\n",
    "1. **Flattening**: Convert 2D particle arrays to 1D feature vectors for MLP input\n",
    "\n",
    "2. **Feature normalization**: Normalize all stream features (positions, velocities) to have zero mean and unit variance\n",
    "   - This improves training stability and convergence speed\n",
    "   - Prevents features with larger magnitudes from dominating the learning\n",
    "\n",
    "3. **Parameter normalization**: Normalize parameters to the range $[-1, 1]$\n",
    "   - Neural Spline Flows (NSF) are designed to work within a bounded region. This normalization is necessary, else the distributions will be unphysically truncated.\n",
    "   - In `zuko`, this is set to $[-5, 5]$ by default, but normalizing to $[-1, 1]$ provides better numerical stability during training.\n",
    "\n",
    "**Note**: We also subsample streams to 1000 particles (from 9998) to reduce computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_mlp(streams, params_dict, train_frac=0.8, batch_size=32):\n",
    "    \"\"\" Prepare training data for MLP + NSF model \"\"\"\n",
    "    N_samples, N_particles, N_features = streams.shape\n",
    "    streams = streams.reshape(N_samples, N_particles * N_features)  # Flatten the streams\n",
    "\n",
    "    # note: in practice, it is better to train on log10(M_sh) instead of M_sh\n",
    "    # because M_sh spans several orders of magnitude\n",
    "    params = np.vstack((np.log10(params_dict['M_sh']), params_dict['vz_impact'])).T  # (N_samples, 2)\n",
    "\n",
    "    # divide the training data into training and validation sets\n",
    "    N_train = int(train_frac * N_samples)\n",
    "    X_train = torch.tensor(streams[:N_train], dtype=torch.float32)\n",
    "    y_train = torch.tensor(params[:N_train], dtype=torch.float32)\n",
    "    X_val = torch.tensor(streams[N_train:], dtype=torch.float32)\n",
    "    y_val = torch.tensor(params[N_train:], dtype=torch.float32)\n",
    "\n",
    "    print('Number of training samples:', X_train.shape[0])\n",
    "    print('Number of validation samples:', X_val.shape[0])\n",
    "\n",
    "    # normalize the inpput and output features\n",
    "    X_loc, X_scale = X_train.mean(0), X_train.std(0)\n",
    "    y_min, y_max = y_train.min(0)[0], y_train.max(0)[0]\n",
    "    y_loc = (y_min + y_max) / 2\n",
    "    y_scale = (y_max - y_min) / 2\n",
    "    X_train = (X_train - X_loc) / X_scale\n",
    "    y_train = (y_train - y_loc) / y_scale\n",
    "    X_val = (X_val - X_loc) / X_scale\n",
    "    y_val = (y_val - y_loc) / y_scale\n",
    "\n",
    "    # create DataLoader for training and validation sets\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    norm_dict = {'X_loc': X_loc, 'X_scale': X_scale, 'y_loc': y_loc, 'y_scale': y_scale}\n",
    "\n",
    "    return train_loader, val_loader, norm_dict\n",
    "\n",
    "def subsample_streams(streams, N_particles_subsampled=100):\n",
    "    \"\"\" Subsample the streams to a fixed number of particles \"\"\"\n",
    "    N_samples, N_particles, N_features = streams.shape\n",
    "    if N_particles_subsampled >= N_particles:\n",
    "        return streams  # no need to subsample\n",
    "\n",
    "    streams_subsampled = np.zeros((N_samples, N_particles_subsampled, N_features))\n",
    "    for i in range(N_samples):\n",
    "        indices = np.random.choice(N_particles, N_particles_subsampled, replace=False)\n",
    "        streams_subsampled[i] = streams[i, indices]\n",
    "\n",
    "    return streams_subsampled\n",
    "\n",
    "# subsample to reduce computational load\n",
    "streams_subsampled = subsample_streams(streams, N_particles_subsampled=100)\n",
    "train_loader, val_loader, norm_dict = prepare_training_mlp(\n",
    "    streams_subsampled, params_dict, train_frac=0.9, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and Train the NPE Model\n",
    "\n",
    "Now we'll create our NPE model and train it. The model consists of two main components:\n",
    "\n",
    "1. **MLP Embedding Network**: Compresses the high-dimensional stream data (3000 features from 1000 particles × 3 positions) into a low-dimensional embedding (8 dimensions)\n",
    "2. **Neural Spline Flow**: A normalizing flow that learns the conditional posterior distribution $p(\\theta|x)$ given the embedding\n",
    "\n",
    "**Training Details:**\n",
    "- We use negative log-likelihood as the loss function (maximize $\\log p(\\theta|x)$)\n",
    "- AdamW optimizer with weight decay for regularization\n",
    "- Monitor both training and validation loss to detect overfitting\n",
    "\n",
    "For more information on the NPE training procedure, please refer to Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import simple_npe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=50, lr=1e-3, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the NPE model given training and validation data loaders.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    print(f\"Training for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "        for x_batch, theta_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            log_prob = model.log_prob(theta_batch.to(device), x_batch.to(device))\n",
    "            loss = -log_prob.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # compute the validation loss at the end of each epoch\n",
    "        model.eval()\n",
    "        val_epoch_loss = 0.0\n",
    "        n_val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for x_val_batch, theta_val_batch in val_loader:\n",
    "                val_log_prob = model.log_prob(theta_val_batch.to(device), x_val_batch.to(device))\n",
    "                val_loss = -val_log_prob.mean()\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                n_val_batches += 1\n",
    "        val_avg_loss = val_epoch_loss / n_val_batches\n",
    "        val_losses.append(val_avg_loss)\n",
    "\n",
    "        # print progress every epoch\n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_npe = simple_npe.SimpleNPE(\n",
    "    input_dim=streams_subsampled.shape[1] * streams_subsampled.shape[2],  # (100 particles * 6 features)\n",
    "    output_dim=2,\n",
    "    hidden_size=64,\n",
    "    embedding_size=8,\n",
    "    transforms=6\n",
    ")\n",
    "\n",
    "# note: I set epoch to 20 to save time during testing, which I find sufficient for the model\n",
    "# to converge. In practice, consider training for more epochs\n",
    "train_losses, val_losses = train_model(\n",
    "    simple_npe, train_loader, val_loader, epochs=20, lr=1e-4,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'  # use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.plot(train_losses, label='Train Loss')\n",
    "ax.plot(val_losses, label='Validation Loss')\n",
    "ax.set_xlabel('Epoch', fontsize=16)\n",
    "ax.set_ylabel('Negative Log-Likelihood', fontsize=16)\n",
    "ax.legend(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Posterior Samples with Corner Plots\n",
    "\n",
    "Now that our model is trained, let's test it on some validation examples by sampling from the posterior $p(\\theta|x)$ and visualizing the results with corner plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install corner for visualization if needed\n",
    "try:\n",
    "    import corner\n",
    "    print(\"corner library is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install corner\n",
    "    import corner\n",
    "    print(\"corner library installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_obs(model, x_obs, norm_dict, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Sample from the posterior p(θ|x) for a given observation.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained NPE model\n",
    "    - x_obs: tensor containing the observation x\n",
    "    - norm_dict: dictionary containing normalization parameters\n",
    "    - n_samples: number of samples to draw from the posterior\n",
    "    Returns:\n",
    "    - samples: numpy array of shape (n_samples, 2) containing samples from the posterior\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    x_obs = torch.tensor(x_obs, dtype=torch.float32)\n",
    "    x_obs = x_obs.reshape(1, -1)  # reshape to (1, input_dim)\n",
    "    x_obs = (x_obs - norm_dict['X_loc']) / norm_dict['X_scale']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        samples_normalized = model.sample(x_obs, n_samples)  # zuko syntax\n",
    "        samples = samples_normalized.cpu().numpy().squeeze()\n",
    "        samples = samples * norm_dict['y_scale'].numpy() + norm_dict['y_loc'].numpy()\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few test examples from validation set\n",
    "n_test_examples = 3\n",
    "test_indices = [9000, 9200, 9500]  # indices from validation set\n",
    "\n",
    "# Prepare test data (same preprocessing as training data)\n",
    "test_streams = streams_subsampled[test_indices]\n",
    "test_params = np.vstack((np.log10(M_sh[test_indices]), vz_impact[test_indices])).T\n",
    "\n",
    "print(f\"Sampling posteriors for {n_test_examples} test examples...\")\n",
    "\n",
    "# Sample from posterior for each test example\n",
    "posterior_samples = []\n",
    "for i, idx in enumerate(test_indices):\n",
    "    samples = sample_single_obs(simple_npe, test_streams[i], norm_dict, n_samples=10_000)\n",
    "    posterior_samples.append(samples)\n",
    "    print(f\"- Example {i+1}: log10(M_sh)={test_params[i, 0]:.2f}, vz={test_params[i, 1]:.1f} km/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the posterior samples using corner\n",
    "for i in range(n_test_examples):\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    fig = corner.corner(\n",
    "        posterior_samples[i],\n",
    "        labels=[r'$\\log_{10}(M_\\mathrm{sh}/\\mathrm{M}_{\\odot})$', r'$v_{z}$ [km/s]'],\n",
    "        truths=test_params[i],\n",
    "        show_titles=True,\n",
    "        title_fmt='.2f',\n",
    "        title_kwargs={\"fontsize\": 14},\n",
    "        label_kwargs={\"fontsize\": 14},\n",
    "        bins=20,\n",
    "        levels=(0.68, 0.95),\n",
    "        quantiles=[0.16, 0.5, 0.84],\n",
    "        smooth=1.0,\n",
    "        smooth1d=1.0,\n",
    "        color='C0',\n",
    "        truth_color='C1',\n",
    "        fig=fig\n",
    "    )\n",
    "    fig.suptitle(f'Example {i+1}', fontsize=14, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Flow Leakage:** When using normalizing flows, be cautious of flow leakage - a phenomenon where the flow assigns non-negligible probability mass to parameter regions outside the prior bounds. This can occur when:\n",
    "- The flow's base distribution (usually Gaussian) has unbounded support while the prior is bounded\n",
    "- Training data doesn't adequately sample the prior boundaries\n",
    "- The flow architecture doesn't enforce hard constraints\n",
    "\n",
    "Flow leakage naturally improves with more training samples, but can still be present in well-trained models. Always check that the posterior samples respect the prior bounds, and if not, simply reject samples outside the prior during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validation Tests\n",
    "\n",
    "A major advantage of NPE is amortization: once trained, the model can infer posteriors for new observations without retraining. This allows us to perform extensive tests on the full validation tests to assess model performance.\n",
    "\n",
    "We will perform two tests:\n",
    "1. **Predicted vs True**: Plot the predicted posterior means against the true parameter values for all validation examples. This gives a quick visual check of bias and scatter.\n",
    "2. **Coverage Tests**: Evaluate how well the predicted posteriors capture the true parameters using rank-based and TARP coverage tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_posteriors(model, loader, norm_dict, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Sample from the posterior p(θ|x) from all observations in the DataLoader.\n",
    "    Also return the true parameters for comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained NPE model\n",
    "    - loader: DataLoader containing the observation x\n",
    "    - norm_dict: dictionary containing normalization parameters\n",
    "    - n_samples: number of samples to draw from the posterior\n",
    "    Returns:\n",
    "    - samples: numpy array of shape (n_samples, 2) containing samples from the posterior\n",
    "    - truths: numpy array of shape (n_samples, 2) containing the true parameters\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    posteriors_list = []\n",
    "    truths_list = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            context = model.embedding_network(x_batch)\n",
    "            samples_normalized = model.flow(context).sample((n_samples, ))  # zuko syntax\n",
    "\n",
    "            # zuko returns (n_samples, batch_size, output_dim)\n",
    "            # we will reshape this to (batch_size, n_samples, output_dim) for easier processing\n",
    "            samples_normalized = samples_normalized.permute(1, 0, 2)\n",
    "\n",
    "            # Denormalize back to original parameter space\n",
    "            samples = samples_normalized.cpu().numpy().squeeze()\n",
    "            samples = samples * norm_dict['y_scale'].numpy() + norm_dict['y_loc'].numpy()\n",
    "            posteriors_list.append(samples)\n",
    "\n",
    "            truths = y_batch.cpu().numpy().squeeze()\n",
    "            truths = truths * norm_dict['y_scale'].numpy() + norm_dict['y_loc'].numpy()\n",
    "            truths_list.append(truths)\n",
    "\n",
    "    posteriors_list = np.concatenate(posteriors_list, axis=0)\n",
    "    truths_list = np.concatenate(truths_list, axis=0)\n",
    "\n",
    "    return posteriors_list, truths_list\n",
    "\n",
    "posterior_samples_all, truths_all = sample_posteriors(\n",
    "    simple_npe, val_loader, norm_dict, n_samples=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first plot the predicted vs true values for both parameters across all validation examples. Ideally, points should lie along the diagonal line, indicating accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Compute point estimates (median) and uncertainties (16th and 84th percentiles)\n",
    "predicted_median, predicted_lower, predicted_upper = np.percentile(\n",
    "    posterior_samples_all, [50, 16, 84], axis=1)\n",
    "\n",
    "# True vs Predicted for log10(M_sh)\n",
    "axes[0].errorbar(\n",
    "    truths_all[:, 0],\n",
    "    predicted_median[:, 0],\n",
    "    yerr=[predicted_median[:, 0] - predicted_lower[:, 0],\n",
    "          predicted_upper[:, 0] - predicted_median[:, 0]],\n",
    "    fmt='o', markersize=4, alpha=0.5, label='Predictions',\n",
    "    zorder=0\n",
    ")\n",
    "axes[0].plot([6, 8], [6, 8], 'k--', lw=2, zorder=1)\n",
    "axes[0].set_xlabel(r'True $\\log_{10}(M_{sh})$', fontsize=16)\n",
    "axes[0].set_ylabel(r'Predicted $\\log_{10}(M_{sh})$', fontsize=16)\n",
    "\n",
    "# True vs Predicted for vz_impact\n",
    "axes[1].errorbar(\n",
    "    truths_all[:, 1],\n",
    "    predicted_median[:, 1],\n",
    "    yerr=[predicted_median[:, 1] - predicted_lower[:, 1],\n",
    "          predicted_upper[:, 1] - predicted_median[:, 1]],\n",
    "    fmt='o', markersize=4, alpha=0.5, label='Predictions',\n",
    "    zorder=0\n",
    ")\n",
    "axes[1].plot([-50, 50], [-50, 50], 'k--', lw=2, zorder=1)\n",
    "axes[1].set_xlabel(r'True $v_z$ [km/s]', fontsize=16)\n",
    "axes[1].set_ylabel(r'Predicted $v_z$ [km/s]', fontsize=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's validate our NPE model using the calibration tests from Tutorial 04:\n",
    "- **Rank-based test**: Checks if marginal posteriors are well-calibrated\n",
    "- **TARP**: Tests joint posterior calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "from scipy import stats\n",
    "\n",
    "# Install tarp for coverage tests if needed\n",
    "try:\n",
    "    import tarp\n",
    "    print(\"tarp library is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install deprecation tarp\n",
    "    import tarp\n",
    "    print(\"tarp library installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Rank-based Test\n",
    "See Tutorial 04 for details on the rank-based calibration test. We'll apply it separately for each parameter, since this test only works for 1D marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_based_calibration(theta_true, theta_samples):\n",
    "    \"\"\"\n",
    "    Compute ranks for the rank-based calibration test.\n",
    "\n",
    "    Parameters:\n",
    "    - theta_true: (n_tests,) array of true parameter values\n",
    "    - theta_samples: (n_tests, n_samples) array of posterior samples\n",
    "\n",
    "    Returns:\n",
    "    - ranks: (n_tests,) array of ranks (normalized to [0, 1])\n",
    "    \"\"\"\n",
    "    n_tests, n_samples = theta_samples.shape\n",
    "\n",
    "    # For each test case, count how many samples are less than the true value\n",
    "    ranks = np.sum(theta_samples < theta_true.reshape(-1, 1), axis=1)\n",
    "\n",
    "    # Normalize ranks to [0, 1]\n",
    "    normalized_ranks = ranks / n_samples\n",
    "\n",
    "    return normalized_ranks\n",
    "\n",
    "# Compute ranks for each parameter separately\n",
    "ranks_log_mass = rank_based_calibration(truths_all[:, 0], posterior_samples_all[:, :, 0])\n",
    "ranks_vz = rank_based_calibration(truths_all[:, 1], posterior_samples_all[:, :, 1])\n",
    "\n",
    "print(\"Rank-based Calibration Statistics:\")\n",
    "print(f\"\\nlog10(M_sh):\")\n",
    "print(f\"  Mean: {ranks_log_mass.mean():.3f} (expected: 0.500)\")\n",
    "print(f\"  Std:  {ranks_log_mass.std():.3f} (expected: {1/np.sqrt(12):.3f})\")\n",
    "print(f\"\\nvz:\")\n",
    "print(f\"  Mean: {ranks_vz.mean():.3f} (expected: 0.500)\")\n",
    "print(f\"  Std:  {ranks_vz.std():.3f} (expected: {1/np.sqrt(12):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "param_names = [r'$\\log_{10}(M_{\\rm sh}/M_{\\odot})$', r'$v_z$ [km/s]']\n",
    "all_ranks = [ranks_log_mass, ranks_vz]\n",
    "\n",
    "for param_idx, (ranks, param_name) in enumerate(zip(all_ranks, param_names)):\n",
    "    # Histogram\n",
    "    ax = axes[param_idx, 0]\n",
    "    ax.hist(ranks, bins=30, color=f'C{param_idx}', density=True, alpha=0.7)\n",
    "    ax.axhline(1.0, color='black', linestyle='--', linewidth=2, label='Uniform (expected)')\n",
    "    ax.set_xlabel('Normalized Rank', fontsize=14)\n",
    "    ax.set_ylabel('Density', fontsize=14)\n",
    "    ax.set_title(f'Rank Distribution: {param_name}', fontsize=14)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_ylim([0, 2.5])\n",
    "\n",
    "    # Q-Q plot\n",
    "    ax = axes[param_idx, 1]\n",
    "    sorted_ranks = np.sort(ranks)\n",
    "    theoretical_quantiles = np.linspace(0, 1, len(sorted_ranks))\n",
    "    ax.plot(theoretical_quantiles, sorted_ranks, '-', lw=2, color=f'C{param_idx}')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "\n",
    "    # Confidence bands (1, 2, 3 sigma)\n",
    "    n = len(ranks)\n",
    "    se = 1 / np.sqrt(12 * n)\n",
    "    sigmas = [1, 2, 3]\n",
    "    alphas = [0.4, 0.3, 0.2]\n",
    "    quantiles = np.linspace(0, 1, 100)\n",
    "    for sigma, alpha_val in zip(sigmas, alphas):\n",
    "        lower_band = np.maximum(quantiles - sigma * se, 0)\n",
    "        upper_band = np.minimum(quantiles + sigma * se, 1)\n",
    "        ax.fill_between(quantiles, lower_band, upper_band, alpha=alpha_val, color='gray')\n",
    "\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_xlabel('Expected Quantiles', fontsize=14)\n",
    "    ax.set_ylabel('Observed Quantiles', fontsize=14)\n",
    "    ax.set_title(f'Q-Q Plot: {param_name}', fontsize=14)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kolmogorov-Smirnov test\n",
    "ks_stat_mass, p_value_mass = kstest(ranks_log_mass, 'uniform')\n",
    "ks_stat_vz, p_value_vz = kstest(ranks_vz, 'uniform')\n",
    "\n",
    "print(f\"\"\"Kolmogorov-Smirnov Test Results\n",
    "(Tests if ranks are uniformly distributed)\n",
    "\n",
    "log10(M_sh):\n",
    "  KS statistic: {ks_stat_mass:.4f}\n",
    "  p-value: {p_value_mass:.4f}\n",
    "  {'✓ Well calibrated' if p_value_mass > 0.05 else '✗ Poor calibration'}\n",
    "\n",
    "vz:\n",
    "  KS statistic: {ks_stat_vz:.4f}\n",
    "  p-value: {p_value_vz:.4f}\n",
    "  {'✓ Well calibrated' if p_value_vz > 0.05 else '✗ Poor calibration'}\n",
    "\n",
    "Note: p-value > 0.05 suggests good calibration\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. TARP Calibration Test\n",
    "\n",
    "See Tutorial 04 for details on TARP ([Lemos et al. 2023](https://arxiv.org/abs/2302.03026)). Unlike the rank-based test, TARP can detect miscalibration in the joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TARP coverage test\n",
    "print(\"Running TARP coverage test...\")\n",
    "print(\"This may take a minute...\\n\")\n",
    "\n",
    "# TARP expects shape: (n_samples, n_tests, n_params)\n",
    "# We have: (n_tests, n_samples, n_params)\n",
    "# So we need to transpose the first two dimensions\n",
    "tarp_samples = np.transpose(posterior_samples_all, (1, 0, 2))\n",
    "\n",
    "ecp, alpha = tarp.get_tarp_coverage(\n",
    "    tarp_samples,  # shape: (n_samples, n_tests, n_params)\n",
    "    truths_all,    # shape: (n_tests, n_params)\n",
    "    bootstrap=True,\n",
    "    num_bootstrap=100\n",
    ")\n",
    "\n",
    "print(\"✓ TARP test complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot TARP Expected Coverage Probability (ECP) curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "# Plot the mean ECP curve\n",
    "ax.plot(alpha, ecp.mean(axis=0), label='Observed Coverage', color='C0', linewidth=3)\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
    "\n",
    "# Plot confidence bands (1, 2, 3 sigma) from bootstrap\n",
    "sigmas = [1, 2, 3]\n",
    "alphas_fill = [0.4, 0.3, 0.2]\n",
    "for k, alpha_val in zip(sigmas, alphas_fill):\n",
    "    ax.fill_between(\n",
    "        alpha,\n",
    "        ecp.mean(axis=0) - k * ecp.std(axis=0),\n",
    "        ecp.mean(axis=0) + k * ecp.std(axis=0),\n",
    "        color='C0', alpha=alpha_val,\n",
    "    )\n",
    "\n",
    "# Add reference curves for over/under-confident\n",
    "x_region = np.linspace(0, 1, 100)\n",
    "overconfident_curve = x_region + 0.15 * np.sin(2 * np.pi * x_region)\n",
    "underconfident_curve = x_region - 0.15 * np.sin(2 * np.pi * x_region)\n",
    "ax.plot(x_region, overconfident_curve, color='red', ls='dotted',\n",
    "        linewidth=2, alpha=0.5, label='Overconfident (example)')\n",
    "ax.plot(x_region, underconfident_curve, color='blue', ls='dashdot',\n",
    "        linewidth=2, alpha=0.5, label='Underconfident (example)')\n",
    "\n",
    "ax.set_xlabel('Credibility Level', fontsize=16)\n",
    "ax.set_ylabel('Expected Coverage Probability', fontsize=16)\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this tutorial, we successfully implemented and validated an NPE model for inferring subhalo properties from stellar stream data. Here are the key takeaways:\n",
    "\n",
    "- **Model architecture**: we use a simple MLP embedding + Neural Spline Flow to model the posterior distribution. The embedding network compresses high-dimensional stream data into a low-dimensional representation, which the flow uses to learn the conditional posterior.\n",
    "- **Validation test**:\n",
    "  - We plot the median posterior predictions against the true parameters to visually assess bias and scatter.\n",
    "  - We perform rank-based and TARP coverage tests to quantitatively evaluate posterior calibration.\n",
    "- We find that the SimpleNPE model does not provide a good constraint on the parameters and also does not pass the coverage test. This can suggest that:\n",
    "  - The model does is not expressive enough to capture the complex relationships in the data\n",
    "  - The number of training samples is insufficient for the model to learn accurate posteriors\n",
    "\n",
    "In this next tutorial, we will explore more advanced architectures that may improve performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
