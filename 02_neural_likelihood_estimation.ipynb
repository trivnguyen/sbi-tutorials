{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Neural Likelihood Estimation (NLE)\n",
    "Credit: Tri Nguyen and Claude Code\n",
    "\n",
    "In this tutorial, we'll implement neural likelihood estimation (NLE) from scratch using PyTorch. NLE is a simulation-based inference method that uses neural networks to approximate the likelihood function $p(x|\\theta)$, which can then be combined with sampling algorithms to obtain the posterior.\n",
    "\n",
    "**Setup**:\n",
    "- Parameter: $\\theta$ sampled from a prior distribution $p(\\theta)$\n",
    "- Likelihood: $p(x|\\theta)$ which we'll learn with a neural network\n",
    "- Observation: $x \\sim p(x|\\theta)$\n",
    "- Posterior: $p(\\theta|x) \\propto p(x|\\theta) p(\\theta)$ (obtained via sampling)\n",
    "\n",
    "**Goal**: Learn the likelihood $p(x|\\theta)$ using a neural network $q_\\phi(x|\\theta)$, then use sampling to obtain the posterior.\n",
    "\n",
    "**Key Difference from Tutorial 1**: In NPE (Tutorial 1), we directly learned $p(\\theta|x)$. In NLE, we learn $p(x|\\theta)$, then use **sampling methods** (MCMC or Nested Sampling) to obtain $p(\\theta|x) \\propto p(x|\\theta)p(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Install sampling packages if needed\n",
    "try:\n",
    "    import emcee\n",
    "    import dynesty\n",
    "    print(\"✓ Sampling packages already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing emcee and dynesty...\")\n",
    "    !pip install emcee dynesty\n",
    "    import emcee\n",
    "    import dynesty\n",
    "    print(\"✓ Sampling packages installed\")\n",
    "\n",
    "from dynesty import NestedSampler\n",
    "from dynesty import plotting as dyplot\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1a: Gaussian Likelihood on Gaussian Data\n",
    "\n",
    "We'll start with a simple case where the true likelihood is Gaussian.\n",
    "We'll implement a simple NLE model where the likelihood is approximated using a Gaussian distribution parameterized by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the training data\n",
    "\n",
    "We'll use the same Gaussian joint distribution from Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian_joint(n_samples, mean=[0, 0], cov=[[1.0, 0.8], [0.8, 1.0]]):\n",
    "    \"\"\"\n",
    "    Sample from a 2D Gaussian joint distribution p(x, θ).\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples: number of samples\n",
    "    - mean: [mean_theta, mean_x]\n",
    "    - cov: 2x2 covariance matrix\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    theta = samples[:, 0]\n",
    "    x = samples[:, 1]\n",
    "    return theta, x\n",
    "\n",
    "# Visualize the Gaussian joint distribution\n",
    "theta_gauss_vis, x_gauss_vis = sample_gaussian_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_gauss_vis, x_gauss_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Gaussian Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_gauss_vis)} points from 2D Gaussian\")\n",
    "print(f\"Correlation between θ and x: {np.corrcoef(theta_gauss_vis, x_gauss_vis)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from Gaussian\n",
    "n_training = 10_000\n",
    "mean = [0, 0]\n",
    "cov = [[1.0, 0.8], [0.8, 1.0]]\n",
    "print(f\"Generating {n_training} training samples from Gaussian...\")\n",
    "\n",
    "theta_train_gauss, x_train_gauss = sample_gaussian_joint(n_training, mean=mean, cov=cov)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  θ shape: {theta_train_gauss.shape}\")\n",
    "print(f\"  x shape: {x_train_gauss.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Gaussian Likelihood Model\n",
    "\n",
    "**Recall from Tutorial 1**: We modeled the posterior as $q_\\phi(\\theta|x) = \\mathcal{N}(\\theta; \\mu_\\phi(x), \\sigma^2_\\phi(x))$, where the input was $x$ and output was parameters for $\\theta$.\n",
    "\n",
    "**In NLE**: We flip this! Now we model the likelihood as $q_\\phi(x|\\theta) = \\mathcal{N}(x; \\mu_\\phi(\\theta), \\sigma^2_\\phi(\\theta))$, where the input is $\\theta$ and output is parameters for $x$.\n",
    "\n",
    "The architecture is identical to Tutorial 1's `GaussianPosterior`, but the roles of $\\theta$ and $x$ are swapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianLikelihood(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network that models p(x|θ) as a Gaussian.\n",
    "\n",
    "    This architecture is identical to Tutorial 1's GaussianPosterior,\n",
    "    but here we input θ and output parameters for x.\n",
    "\n",
    "    Input: θ (parameter)\n",
    "    Output: mean μ(θ) and std σ(θ) for p(x|θ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),  # input dimension is 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Output layers for mean and log std of Gaussian\n",
    "        self.mean_layer = nn.Linear(hidden_size, 1)\n",
    "        self.logstd_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, theta):\n",
    "        \"\"\"Predict Gaussian parameters μ(θ), σ(θ)\"\"\"\n",
    "        features = self.network(theta)\n",
    "        mean = self.mean_layer(features)\n",
    "        logstd = self.logstd_layer(features)\n",
    "        return mean, logstd\n",
    "\n",
    "    def log_prob(self, x, theta):\n",
    "        \"\"\"Compute log p(x|θ)\"\"\"\n",
    "        mean, logstd = self.forward(theta)\n",
    "        var = torch.exp(2 * logstd)\n",
    "        log_prob = -0.5 * (np.log(2 * np.pi) + 2 * logstd + ((x - mean) ** 2) / var)\n",
    "        return log_prob\n",
    "\n",
    "gaussian_likelihood_v1 = GaussianLikelihood(hidden_size=64)\n",
    "print(\"✓ Gaussian likelihood model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "The training objective is the same as Tutorial 1. We minimize the negative log-likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = -\\mathbb{E}_{p(x, \\theta)}[\\log q_\\phi(x|\\theta)]$$\n",
    "\n",
    "where $(x, \\theta) \\sim p(x, \\theta)$ are training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_likelihood_model(model, theta_train, x_train, epochs=50, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the likelihood model.\n",
    "\n",
    "    Note: The dataloader order is (theta, x) since we model p(x|θ).\n",
    "    This is opposite from Tutorial 1 where we had (x, theta) for p(θ|x).\n",
    "    \"\"\"\n",
    "    theta_tensor = torch.tensor(theta_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    x_tensor = torch.tensor(x_train, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    dataset = TensorDataset(theta_tensor, x_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    print(f\"Training for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for theta_batch, x_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            log_prob = model.log_prob(x_batch, theta_batch)\n",
    "            loss = -log_prob.mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    return losses\n",
    "\n",
    "# Train on Gaussian data\n",
    "gauss_lik_losses_v1 = train_likelihood_model(gaussian_likelihood_v1, theta_train_gauss, x_train_gauss, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior Sampling with MCMC and Nested Sampling\n",
    "\n",
    "**Key difference from NPE**: In Tutorial 1 (NPE), we could directly sample from $p(\\theta|x)$ using the trained model. In NLE, we only have $p(x|\\theta)$, so we need to use sampling algorithms to obtain $p(\\theta|x) \\propto p(x|\\theta)p(\\theta)$.\n",
    "\n",
    "We'll implement two sampling approaches:\n",
    "1. **MCMC with emcee**: Ensemble sampler for exploring the posterior\n",
    "2. **Nested Sampling with dynesty**: Computes both posterior samples and evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_fn(theta, x_obs, likelihood_model):\n",
    "    \"\"\"Log-likelihood function for sampling.\"\"\"\n",
    "    theta_tensor = torch.tensor([[theta]], dtype=torch.float32)\n",
    "    x_tensor = torch.tensor([[x_obs]], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        return likelihood_model.log_prob(x_tensor, theta_tensor).item()\n",
    "\n",
    "def log_prior_uniform(theta, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Uniform prior.\"\"\"\n",
    "    if theta_min < theta < theta_max:\n",
    "        return 0.0  # log(1/(theta_max - theta_min)) + constant\n",
    "    return -np.inf\n",
    "\n",
    "def log_posterior_fn(theta, x_obs, likelihood_model, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Log-posterior: log p(θ|x) = log p(x|θ) + log p(θ)\"\"\"\n",
    "    lp = log_prior_uniform(theta, theta_min, theta_max)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood_fn(theta, x_obs, likelihood_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MCMC and Nested Sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_emcee(likelihood_model, x_obs, n_walkers=32, n_steps=3000,\n",
    "                      theta_min=-3, theta_max=3, burn_in=1000):\n",
    "    \"\"\"\n",
    "    Sample from posterior using emcee (MCMC).\n",
    "    \"\"\"\n",
    "    ndim = 1\n",
    "\n",
    "    # Initialize walkers randomly in prior range\n",
    "    p0 = np.random.uniform(theta_min, theta_max, size=(n_walkers, ndim))\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        n_walkers, ndim, log_posterior_fn,\n",
    "        args=(x_obs, likelihood_model, theta_min, theta_max)\n",
    "    )\n",
    "\n",
    "    # Run MCMC\n",
    "    print(f\"  Running emcee with {n_walkers} walkers for {n_steps} steps...\")\n",
    "    sampler.run_mcmc(p0, n_steps, progress=False)\n",
    "\n",
    "    # Get samples (discard burn-in)\n",
    "    samples = sampler.get_chain(discard=burn_in, flat=True)\n",
    "\n",
    "    # Compute acceptance fraction\n",
    "    acc_frac = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"  Mean acceptance fraction: {acc_frac:.2%}\")\n",
    "\n",
    "    return samples.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform(u, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Transform unit cube to prior (for nested sampling).\"\"\"\n",
    "    return theta_min + (theta_max - theta_min) * u\n",
    "\n",
    "def sample_with_dynesty(likelihood_model, x_obs, theta_min=-3, theta_max=3,\n",
    "                       nlive=500, dlogz=0.5):\n",
    "    \"\"\"\n",
    "    Sample from posterior using dynesty (Nested Sampling).\n",
    "    \"\"\"\n",
    "    # Define likelihood for dynesty\n",
    "    def loglike(theta):\n",
    "        return log_likelihood_fn(theta[0], x_obs, likelihood_model)\n",
    "\n",
    "    # Define prior transform\n",
    "    def ptform(u):\n",
    "        return np.array([prior_transform(u[0], theta_min, theta_max)])\n",
    "\n",
    "    # Create nested sampler\n",
    "    print(f\"  Running dynesty with {nlive} live points...\")\n",
    "    sampler = NestedSampler(loglike, ptform, ndim=1, nlive=nlive)\n",
    "    sampler.run_nested(dlogz=dlogz, print_progress=False)\n",
    "\n",
    "    results = sampler.results\n",
    "\n",
    "    # Get weighted posterior samples\n",
    "    weights = np.exp(results['logwt'] - results['logz'][-1])\n",
    "    samples = dynesty.utils.resample_equal(results.samples, weights)\n",
    "\n",
    "    print(f\"  Log-evidence: {results.logz[-1]:.2f} ± {results.logzerr[-1]:.2f}\")\n",
    "\n",
    "    return samples.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate: Compare MCMC and Nested Sampling\n",
    "\n",
    "Let's see how both samplers perform on the Gaussian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_comparison(likelihood_model, x_val, theta_train, x_train,\n",
    "                              theta_min=-3, theta_max=3):\n",
    "    \"\"\"\n",
    "    Compare MCMC and Nested Sampling for a given x value.\n",
    "\n",
    "    Similar to Tutorial 1's plot_posterior_comparison, but here we need to\n",
    "    run sampling algorithms since we have p(x|θ) instead of p(θ|x).\n",
    "    \"\"\"\n",
    "    print(f\"\\nSampling posterior for x={x_val:.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # MCMC sampling\n",
    "    print(\"[1/2] MCMC (emcee):\")\n",
    "    mcmc_samples = sample_with_emcee(likelihood_model, x_val,\n",
    "                                     theta_min=theta_min, theta_max=theta_max)\n",
    "\n",
    "    # Nested sampling\n",
    "    print(\"\\n[2/2] Nested Sampling (dynesty):\")\n",
    "    ns_samples = sample_with_dynesty(likelihood_model, x_val,\n",
    "                                     theta_min=theta_min, theta_max=theta_max)\n",
    "\n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # True posterior (from training data)\n",
    "    x_margin = 0.15\n",
    "    mask = np.abs(x_train - x_val) < x_margin\n",
    "    theta_true = theta_train[mask]\n",
    "\n",
    "    # MCMC\n",
    "    axes[0].hist(mcmc_samples, bins=50, density=True, histtype='step',\n",
    "                label='MCMC', color='C0', lw=4)\n",
    "    axes[0].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[0].set_ylabel('Density', fontsize=16)\n",
    "    axes[0].set_title(f'MCMC: $p(\\\\theta|x={x_val:.2f})$', fontsize=16)\n",
    "    axes[0].legend(fontsize=12)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Nested Sampling\n",
    "    axes[1].hist(ns_samples, bins=50, density=True, histtype='step',\n",
    "                label='Nested Sampling', color='C1', lw=4)\n",
    "    axes[1].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[1].set_ylabel('Density', fontsize=16)\n",
    "    axes[1].set_title(f'Nested Sampling: $p(\\\\theta|x={x_val:.2f})$', fontsize=16)\n",
    "    axes[1].legend(fontsize=12)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    # Overlay comparison\n",
    "    axes[2].hist(mcmc_samples, bins=50, density=True, alpha=0.8,\n",
    "                label='MCMC', color='C0', histtype='step', lw=4)\n",
    "    axes[2].hist(ns_samples, bins=50, density=True, alpha=0.8,\n",
    "                label='Nested Sampling', color='C1', histtype='step', lw=4)\n",
    "    axes[2].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[2].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[2].set_ylabel('Density', fontsize=16)\n",
    "    axes[2].set_title('Comparison', fontsize=16)\n",
    "    axes[2].legend(fontsize=12)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mcmc_samples, ns_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Gaussian data\n",
    "test_x = 0.5\n",
    "mcmc_gauss, ns_gauss = plot_posterior_comparison(\n",
    "    gaussian_likelihood_v1, test_x, theta_train_gauss, x_train_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1b: Gaussian Likelihood on Half-Moon Data\n",
    "\n",
    "Now let's see what happens when we apply the **same Gaussian likelihood** to **non-Gaussian** data! We'll use the half-moon shaped $p(x, \\theta)$ from Tutorial 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_halfmoon_joint(n_samples, noise=0.1):\n",
    "    \"\"\"\n",
    "    Sample from half-moon joint distribution p(x, θ).\n",
    "\n",
    "    Same data generator as Tutorial 1.\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples, labels = make_moons(n_samples=n_samples, noise=noise, random_state=None)\n",
    "    theta = samples[:, 0]  # First dimension\n",
    "    x = samples[:, 1]      # Second dimension\n",
    "    return theta, x\n",
    "\n",
    "# Visualize\n",
    "theta_moon_vis, x_moon_vis = sample_halfmoon_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_moon_vis, x_moon_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Half-Moon Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_moon_vis)} points from half-moon\")\n",
    "print(f\"θ range: [{theta_moon_vis.min():.2f}, {theta_moon_vis.max():.2f}]\")\n",
    "print(f\"x range: [{x_moon_vis.min():.2f}, {x_moon_vis.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from half-moon\n",
    "n_training = 10_000\n",
    "print(f\"Generating {n_training} training samples from half-moon...\")\n",
    "\n",
    "theta_train_moon, x_train_moon = sample_halfmoon_joint(n_training, noise=0.1)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  θ shape: {theta_train_moon.shape}\")\n",
    "print(f\"  x shape: {x_train_moon.shape}\")\n",
    "\n",
    "# Train Gaussian likelihood on half-moon\n",
    "gaussian_likelihood_v2 = GaussianLikelihood(hidden_size=64)\n",
    "print(\"\\nTraining Gaussian likelihood on half-moon data...\")\n",
    "gauss_lik_losses_v2 = train_likelihood_model(gaussian_likelihood_v2, theta_train_moon, x_train_moon, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate: Gaussian Likelihood on Half-Moon\n",
    "\n",
    "Let's see how the Gaussian likelihood performs with sampling on non-Gaussian data. Just like in Tutorial 1, we expect the Gaussian model to struggle here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on half-moon data\n",
    "test_x_moon = 0.5\n",
    "mcmc_moon_gauss, ns_moon_gauss = plot_posterior_comparison(\n",
    "    gaussian_likelihood_v2, test_x_moon, theta_train_moon, x_train_moon,\n",
    "    theta_min=-1, theta_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the prior distribution $p(\\theta)$ extends beyond the range of the training data. If the model does not learn the correct likelihood in these regions (as we have seen in this case), the posterior estimates will be poor. \n",
    "\n",
    "In practice, one should ensure that the prior distribution is well-aligned with the training data to avoid such issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Flow-based Likelihood\n",
    "\n",
    "Since the Gaussian likelihood **failed** on the half-moon data (just like in Tutorial 1), we need a more flexible likelihood model. We'll use normalizing flows!\n",
    "\n",
    "**Recall from Tutorial 1**: Flows learn an invertible transformation $\\theta = T(z; x)$ where $z \\sim \\mathcal{N}(0, 1)$ to model complex posteriors $p(\\theta|x)$.\n",
    "\n",
    "**In NLE**: We do the same thing, but flipped! We model $x = T(z; \\theta)$ to learn the likelihood $p(x|\\theta)$. Same flexible architecture, just swapped roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install zuko\n",
    "try:\n",
    "    import zuko\n",
    "    print(\"✓ zuko already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing zuko...\")\n",
    "    !pip install zuko\n",
    "    import zuko\n",
    "\n",
    "from zuko.flows import NSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowLikelihood(nn.Module):\n",
    "    \"\"\"\n",
    "    Flow-based model for p(x|θ) using Neural Spline Flows.\n",
    "\n",
    "    This architecture is identical to Tutorial 1's FlowPosterior,\n",
    "    but here we input θ (as context) and model the distribution over x.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64, embedding_size=8, transforms=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embed θ before passing to the flow\n",
    "        self.embedding_network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),  # input dimension is 1 (θ)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embedding_size),\n",
    "        )\n",
    "\n",
    "        # NSF: Neural Spline Flow\n",
    "        # models x conditioned on embedded θ\n",
    "        self.flow = NSF(\n",
    "            features=1,                 # dimension of x\n",
    "            context=embedding_size,     # dimension of embedded θ\n",
    "            transforms=transforms,      # number of transformations\n",
    "            hidden_features=[hidden_size, hidden_size],\n",
    "        )\n",
    "\n",
    "    def forward(self, theta):\n",
    "        \"\"\"Return the flow conditioned on θ\"\"\"\n",
    "        theta_embedded = self.embedding_network(theta)\n",
    "        return self.flow(theta_embedded)\n",
    "\n",
    "    def log_prob(self, x, theta):\n",
    "        \"\"\"Compute log p(x|θ)\"\"\"\n",
    "        theta_embedded = self.embedding_network(theta)\n",
    "        return self.flow(theta_embedded).log_prob(x)\n",
    "\n",
    "flow_likelihood = FlowLikelihood(hidden_size=64, embedding_size=8, transforms=5)\n",
    "print(\"✓ Flow-based likelihood model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train flow likelihood on half-moon data\n",
    "print(\"Training flow likelihood on half-moon data...\")\n",
    "flow_lik_losses = train_likelihood_model(flow_likelihood, theta_train_moon, x_train_moon, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare losses: Gaussian vs Flow\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(gauss_lik_losses_v2, linewidth=2, label='Gaussian')\n",
    "ax.plot(flow_lik_losses, linewidth=2, label='Flow')\n",
    "ax.set_xlabel('Epoch', fontsize=16)\n",
    "ax.set_ylabel('Negative Log-Likelihood', fontsize=16)\n",
    "ax.set_title('Training Loss: Gaussian vs Flow (Half-Moon)', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss - Gaussian: {gauss_lik_losses_v2[-1]:.4f}\")\n",
    "print(f\"Final loss - Flow: {flow_lik_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {gauss_lik_losses_v2[-1] - flow_lik_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate: Flow Likelihood on Half-Moon\n",
    "\n",
    "The flow should capture the complex likelihood much better! Let's test with both MCMC and Nested Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test flow likelihood with both samplers\n",
    "test_x_moon = 0.5\n",
    "mcmc_moon_flow, ns_moon_flow = plot_posterior_comparison(\n",
    "    flow_likelihood, test_x_moon, theta_train_moon, x_train_moon,\n",
    "    theta_min=-1, theta_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow-based likelihood yields better posterior estimates on the half-moon data compared to the Gaussian likelihood. However, as seen before, because the prior extends beyond the training data, the posterior estimates can still be poor in those regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Multiple Observations\n",
    "\n",
    "Let's test the flow likelihood on multiple x values to see the pattern across different observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on multiple x values\n",
    "for x_test in [-0.3, 0.0, 0.3, 0.8]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Testing x = {x_test:.2f}\")\n",
    "    print(\"=\"*70)\n",
    "    mcmc, ns = plot_posterior_comparison(\n",
    "        flow_likelihood, x_test, theta_train_moon, x_train_moon,\n",
    "        theta_min=-1, theta_max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we explored NLE with two different sampling methods and demonstrated why normalizing flows have become the standard method for likelihood modeling.\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**Part 1a: Gaussian Likelihood on Gaussian Data**\n",
    "- Modeled $p(x|\\theta) = \\mathcal{N}(x; \\mu(\\theta), \\sigma^2(\\theta))$ using a simple neural network\n",
    "- Worked well because the true likelihood was Gaussian\n",
    "- Both MCMC and Nested Sampling recovered accurate posteriors\n",
    "\n",
    "**Part 1b: Gaussian Likelihood on Half-Moon Data**\n",
    "- Applied the same Gaussian model to non-Gaussian data\n",
    "- **Failed** to capture the complex curved structure (same failure as Tutorial 1)\n",
    "- Sampling algorithms still work, but produce incorrect posteriors due to the misspecified likelihood\n",
    "\n",
    "**Part 2: Flow-based Likelihood**\n",
    "- Used normalizing flows (Neural Spline Flows) to model $p(x|\\theta)$\n",
    "- Successfully captured the half-moon structure\n",
    "- Significantly lower training loss (showing better fit)\n",
    "- Both MCMC and Nested Sampling produce accurate posterior samples\n",
    "\n",
    "### Key Differences: NPE (Tutorial 1) vs NLE (Tutorial 2)\n",
    "\n",
    "| Aspect | NPE (Tutorial 1) | NLE (Tutorial 2) |\n",
    "|--------|------------------|------------------|\n",
    "| **What we learn** | $p(\\theta\\|x)$ directly | $p(x\\|\\theta)$ |\n",
    "| **Inference** | Direct sampling from model | Requires MCMC/Nested Sampling |\n",
    "| **Flexibility** | Posterior must be representable by model | Any posterior shape (via sampling) |\n",
    "| **Speed** | Fast (direct sampling) | Slower (need sampling) |\n",
    "| **Model architecture** | Input: $x$, Output: $\\theta$ | Input: $\\theta$, Output: $x$ |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**For NLE applications, use normalizing flows for the likelihood.**\n",
    "\n",
    "The benefits mirror Tutorial 1's findings:\n",
    "- **Flexibility**: Can represent arbitrary likelihood distributions\n",
    "- **Reliability**: Work well even when likelihood structure is unknown  \n",
    "- **Standard practice**: The de facto choice in modern simulation-based inference\n",
    "\n",
    "### Choosing Between Sampling Methods\n",
    "\n",
    "- **MCMC (emcee)**: Fast and efficient for simple, unimodal posteriors\n",
    "- **Nested Sampling (dynesty)**: More robust for multimodal/complex posteriors, also provides evidence estimates\n",
    "\n",
    "### When to Use NLE vs NPE?\n",
    "\n",
    "- **Use NPE (Tutorial 1)** when you want fast inference and can afford flexible posterior models\n",
    "- **Use NLE (Tutorial 2)** when the likelihood is more natural to model, or when you need evidence estimates from nested sampling\n",
    "\n",
    "In practice, **NPE is often preferred** because direct posterior sampling is faster. However, NLE remains important for cases where likelihood modeling is more intuitive or when you need the flexibility of choosing different sampling algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
