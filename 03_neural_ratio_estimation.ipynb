{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Neural Ratio Estimation (NRE)\n",
    "Credit: Tri Nguyen and Claude Code\n",
    "\n",
    "In this tutorial, we'll implement neural ratio estimation (NRE) from scratch using PyTorch. NRE is a simulation-based inference method that uses a binary classifier to distinguish between samples from the joint distribution $p(x, \\theta)$ and samples from the product of marginals $p(x)p(\\theta)$.\n",
    "\n",
    "**Setup**:\n",
    "- Parameter: $\\theta$ sampled from a prior distribution $p(\\theta)$\n",
    "- Simulator: $p(x|\\theta)$ which implicitly defines the likelihood function\n",
    "- Observation: $x \\sim p(x|\\theta)$\n",
    "- Posterior: $p(\\theta|x)$ (obtained via the learned ratio and sampling)\n",
    "\n",
    "**Goal**: Learn a classifier $d_\\phi(x, \\theta)$ that distinguishes joint samples from independent samples. The classifier output is related to the posterior via:\n",
    "\n",
    "$$p(\\theta|x) \\propto \\frac{d_\\phi(x, \\theta)}{1 - d_\\phi(x, \\theta)} \\cdot p(\\theta)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$d_\\phi(x, \\theta) = \\frac{p(x, \\theta)}{p(x, \\theta)+ p(x)p(\\theta)} = \\frac{p(x |\\theta)}{p(x |\\theta) + p(x)}$$\n",
    "\n",
    "**Key Difference from Previous Tutorials**:\n",
    "- **Tutorial 1 (NPE)**: Directly learned $p(\\theta|x)$, then sampled directly from the model\n",
    "- **Tutorial 2 (NLE)**: Learned $p(x|\\theta)$, then used MCMC/Nested Sampling\n",
    "- **Tutorial 3 (NRE)**: Learns to classify joint vs independent samples, then uses MCMC/Nested Sampling\n",
    "\n",
    "**Why NRE?** NRE can be more sample-efficient than NPE/NLE and doesn't require explicit density models or normalizing flows. However, it does require generating \"negative\" samples from the product of marginals $p(x)p(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Install sampling packages if needed\n",
    "try:\n",
    "    import emcee\n",
    "    import dynesty\n",
    "    print(\"✓ Sampling packages already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing emcee and dynesty...\")\n",
    "    !pip install emcee dynesty\n",
    "    import emcee\n",
    "    import dynesty\n",
    "    print(\"✓ Sampling packages installed\")\n",
    "\n",
    "from dynesty import NestedSampler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Ratio Estimation on Gaussian Data\n",
    "\n",
    "We'll start with a simple case where the true distribution is Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the training data\n",
    "\n",
    "We'll use the same Gaussian joint distribution from Tutorials 1 and 2. However, for NRE, we also need to generate **negative samples** from the product of marginals $p(x)p(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gaussian_joint(n_samples, mean=[0, 0], cov=[[1.0, 0.8], [0.8, 1.0]]):\n",
    "    \"\"\"\n",
    "    Sample from a 2D Gaussian joint distribution p(x, θ).\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples: number of samples\n",
    "    - mean: [mean_theta, mean_x]\n",
    "    - cov: 2x2 covariance matrix\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    theta = samples[:, 0]\n",
    "    x = samples[:, 1]\n",
    "    return theta, x\n",
    "\n",
    "# Visualize the Gaussian joint distribution\n",
    "theta_gauss_vis, x_gauss_vis = sample_gaussian_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_gauss_vis, x_gauss_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Gaussian Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_gauss_vis)} points from 2D Gaussian\")\n",
    "print(f\"Correlation between θ and x: {np.corrcoef(theta_gauss_vis, x_gauss_vis)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from Gaussian\n",
    "n_training = 10_000\n",
    "mean = [0, 0]\n",
    "cov = [[1.0, 0.8], [0.8, 1.0]]\n",
    "print(f\"Generating {n_training} training samples from Gaussian...\\n\")\n",
    "\n",
    "# Positive samples: from joint p(x, θ)\n",
    "theta_joint, x_joint = sample_gaussian_joint(n_training, mean=mean, cov=cov)\n",
    "\n",
    "# Negative samples: from product of marginals p(x)p(θ)\n",
    "# We can generate these by randomly shuffling one of the dimensions\n",
    "theta_indep = theta_joint.copy()\n",
    "x_indep = np.random.permutation(x_joint)  # shuffle x to break correlation\n",
    "\n",
    "print(f\"  Training data generated\")\n",
    "print(f\"  Joint samples: {len(theta_joint)}\")\n",
    "print(f\"  Independent samples: {len(theta_indep)}\")\n",
    "print(f\"\\nCorrelation in joint samples: {np.corrcoef(theta_joint, x_joint)[0,1]:.3f}\")\n",
    "print(f\"Correlation in independent samples: {np.corrcoef(theta_indep, x_indep)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize joint vs independent samples\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(theta_joint[:1000], x_joint[:1000], alpha=0.4, s=10, color='C0', label='Joint')\n",
    "axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[0].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[0].set_title(r'Joint $p(x, \\theta)$ (Positive samples)', fontsize=16)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "axes[1].scatter(theta_indep[:1000], x_indep[:1000], alpha=0.4, s=10, color='C3', label='Independent')\n",
    "axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[1].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[1].set_title(r'Independent $p(x)p(\\theta)$ (Negative samples)', fontsize=16)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Ratio Estimator\n",
    "\n",
    "The ratio estimator is a **binary classifier** that takes $(x, \\theta)$ as input and outputs a probability:\n",
    "\n",
    "$$d_\\phi(x, \\theta) = p(\\text{joint} | x, \\theta)$$\n",
    "\n",
    "This probability is related to the density ratio:\n",
    "\n",
    "$$d_\\phi(x, \\theta) = \\frac{p(x, \\theta)}{p(x, \\theta) + p(x)p(\\theta)}$$\n",
    "\n",
    "From this, we can recover the likelihood ratio:\n",
    "\n",
    "$$r(x, \\theta) = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac{d_\\phi(x, \\theta)}{1 - d_\\phi(x, \\theta)}$$\n",
    "\n",
    "And since $p(x, \\theta) = p(x|\\theta)p(\\theta)$ and $p(x)p(\\theta) = p(x)p(\\theta)$:\n",
    "\n",
    "$$r(x, \\theta) = \\frac{p(x|\\theta)}{p(x)} \\propto p(\\theta|x) / p(\\theta)$$\n",
    "\n",
    "Therefore, we can compute the unnormalized posterior:\n",
    "\n",
    "$$p(\\theta|x) \\propto r(x, \\theta) \\cdot p(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RatioEstimator(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classifier for ratio estimation.\n",
    "\n",
    "    Input: (x, θ) concatenated\n",
    "    Output: probability that the sample is from the joint p(x, θ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2, hidden_size),  # input: [x, θ] concatenated\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()  # output probability in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, theta):\n",
    "        \"\"\"Compute d(x, θ) = p(joint | x, θ)\"\"\"\n",
    "        inputs = torch.cat([x, theta], dim=1)\n",
    "        return self.network(inputs)\n",
    "\n",
    "    def log_ratio(self, x, theta):\n",
    "        \"\"\"Compute log r(x, θ) = log[p(x,θ) / p(x)p(θ)]\"\"\"\n",
    "        d = self.forward(x, theta)\n",
    "        # Numerical stability: clamp d away from 0 and 1\n",
    "        d = torch.clamp(d, 1e-7, 1 - 1e-7)\n",
    "        return torch.log(d) - torch.log(1 - d)\n",
    "\n",
    "ratio_estimator_gauss = RatioEstimator(hidden_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "We train the classifier using binary cross-entropy loss:\n",
    "\n",
    "$$\\mathcal{L}(\\phi) = -\\mathbb{E}_{p(x,\\theta)}[\\log d_\\phi(x, \\theta)] - \\mathbb{E}_{p(x)p(\\theta)}[\\log(1 - d_\\phi(x, \\theta))]$$\n",
    "\n",
    "where the first term is over joint samples (label=1) and the second term is over independent samples (label=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratio_estimator(model, theta_joint, x_joint, theta_indep, x_indep,\n",
    "                         epochs=50, batch_size=256, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the ratio estimator (binary classifier).\n",
    "    \"\"\"\n",
    "    # Prepare data: joint samples (label=1) and independent samples (label=0)\n",
    "    theta_all = np.concatenate([theta_joint, theta_indep])\n",
    "    x_all = np.concatenate([x_joint, x_indep])\n",
    "    labels = np.concatenate([np.ones(len(theta_joint)), np.zeros(len(theta_indep))])\n",
    "\n",
    "    # Convert to tensors\n",
    "    theta_tensor = torch.tensor(theta_all, dtype=torch.float32).reshape(-1, 1)\n",
    "    x_tensor = torch.tensor(x_all, dtype=torch.float32).reshape(-1, 1)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, theta_tensor, labels_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Instead of the negative log-likelihood, we use binary cross-entropy loss\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    print(f\"Training for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0   # Cumulative loss for the epoch\n",
    "        epoch_correct = 0   # Number of correct predictions\n",
    "        epoch_total = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x_batch, theta_batch, label_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            pred = model(x_batch, theta_batch)\n",
    "            loss = criterion(pred, label_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += ((pred > 0.5).float() == label_batch).sum().item()\n",
    "            epoch_total += len(label_batch)\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        accuracy = epoch_correct / epoch_total\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{epochs} | Loss: {avg_loss:.4f} | Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    print(\"\\n✓ Training complete!\")\n",
    "    return losses, accuracies\n",
    "\n",
    "# Train on Gaussian data\n",
    "losses_gauss, accs_gauss = train_ratio_estimator(\n",
    "    ratio_estimator_gauss, theta_joint, x_joint, theta_indep, x_indep, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(losses_gauss, linewidth=2, color='C0')\n",
    "axes[0].set_xlabel('Epoch', fontsize=16)\n",
    "axes[0].set_ylabel('Binary Cross-Entropy Loss', fontsize=16)\n",
    "axes[0].set_title('Training Loss (Gaussian Data)', fontsize=16)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(accs_gauss, linewidth=2, color='C1')\n",
    "axes[1].set_xlabel('Epoch', fontsize=16)\n",
    "axes[1].set_ylabel('Classification Accuracy', fontsize=16)\n",
    "axes[1].set_title('Classification Accuracy (Gaussian Data)', fontsize=16)\n",
    "axes[1].axhline(0.5, color='k', linestyle='--', label='Random guess', alpha=0.5)\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Classification accuracy is not the main objective.\")\n",
    "print(\"We focus on how well the learned ratio approximates the true posterior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Posterior Sampling with MCMC and Nested Sampling\n",
    "\n",
    "**Key difference from NPE**: Like NLE (Tutorial 2), we need to use **sampling algorithms** to obtain the posterior. We have the ratio $r(x, \\theta)$, which gives us the unnormalized posterior $p(\\theta|x) \\propto r(x, \\theta) \\cdot p(\\theta)$.\n",
    "\n",
    "We'll implement two sampling approaches:\n",
    "1. **MCMC with emcee**: Ensemble sampler for exploring the posterior\n",
    "2. **Nested Sampling with dynesty**: Computes both posterior samples and evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_ratio_fn(theta, x_obs, model):\n",
    "    \"\"\"Compute the ratio r(x_obs, θ) using the trained ratio estimator.\"\"\"\n",
    "    theta_tensor = torch.tensor([theta], dtype=torch.float32).reshape(-1, 1)\n",
    "    x_tensor = torch.tensor([x_obs], dtype=torch.float32).reshape(-1, 1)\n",
    "    with torch.no_grad():\n",
    "        log_r = model.log_ratio(x_tensor, theta_tensor).item()\n",
    "    return log_r\n",
    "\n",
    "def log_prior_uniform(theta, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Uniform prior.\"\"\"\n",
    "    if theta_min < theta < theta_max:\n",
    "        return 0.0  # log(1/(theta_max - theta_min)) + constant\n",
    "    return -np.inf\n",
    "\n",
    "def log_posterior_fn(theta, x_obs, model, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Log-posterior: log p(θ|x) = log r(x, θ) + log p(θ)\"\"\"\n",
    "    lp = log_prior_uniform(theta, theta_min, theta_max)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood_ratio_fn(theta, x_obs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sampling function, this is similar to Tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_emcee(model, x_obs, n_walkers=32, n_steps=3000,\n",
    "                      theta_min=-3, theta_max=3, burn_in=1000):\n",
    "    \"\"\"\n",
    "    Sample from posterior using emcee (MCMC).\n",
    "    \"\"\"\n",
    "    ndim = 1\n",
    "\n",
    "    # Initialize walkers randomly in prior range\n",
    "    p0 = np.random.uniform(theta_min, theta_max, size=(n_walkers, ndim))\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = emcee.EnsembleSampler(\n",
    "        n_walkers, ndim, log_posterior_fn,\n",
    "        args=(x_obs, model, theta_min, theta_max)\n",
    "    )\n",
    "\n",
    "    # Run MCMC\n",
    "    print(f\"  Running emcee with {n_walkers} walkers for {n_steps} steps...\")\n",
    "    sampler.run_mcmc(p0, n_steps, progress=False)\n",
    "\n",
    "    # Get samples (discard burn-in)\n",
    "    samples = sampler.get_chain(discard=burn_in, flat=True)\n",
    "\n",
    "    # Compute acceptance fraction\n",
    "    acc_frac = np.mean(sampler.acceptance_fraction)\n",
    "    print(f\"  Mean acceptance fraction: {acc_frac:.2%}\")\n",
    "\n",
    "    return samples.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_transform(u, theta_min=-3, theta_max=3):\n",
    "    \"\"\"Transform unit cube to prior (for nested sampling).\"\"\"\n",
    "    return theta_min + (theta_max - theta_min) * u\n",
    "\n",
    "def sample_with_dynesty(model, x_obs, theta_min=-3, theta_max=3,\n",
    "                       nlive=500, dlogz=0.5):\n",
    "    \"\"\"\n",
    "    Sample from posterior using dynesty (Nested Sampling).\n",
    "    \"\"\"\n",
    "    # Define likelihood for dynesty\n",
    "    def loglike(theta):\n",
    "        return log_likelihood_ratio_fn(theta[0], x_obs, model)\n",
    "\n",
    "    # Define prior transform\n",
    "    def ptform(u):\n",
    "        return np.array([prior_transform(u[0], theta_min, theta_max)])\n",
    "\n",
    "    # Create nested sampler\n",
    "    print(f\"  Running dynesty with {nlive} live points...\")\n",
    "    sampler = NestedSampler(loglike, ptform, ndim=1, nlive=nlive)\n",
    "    sampler.run_nested(dlogz=dlogz, print_progress=False)\n",
    "\n",
    "    results = sampler.results\n",
    "\n",
    "    # Get weighted posterior samples\n",
    "    weights = np.exp(results['logwt'] - results['logz'][-1])\n",
    "    samples = dynesty.utils.resample_equal(results.samples, weights)\n",
    "\n",
    "    print(f\"  Log-evidence: {results.logz[-1]:.2f} ± {results.logzerr[-1]:.2f}\")\n",
    "\n",
    "    return samples.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior_comparison(model, x_val, theta_train, x_train,\n",
    "                              theta_min=-3, theta_max=3):\n",
    "    \"\"\"\n",
    "    Compare MCMC and Nested Sampling for a given x value.\n",
    "\n",
    "    Similar to Tutorial 1's plot_posterior_comparison, but here we need to\n",
    "    run sampling algorithms since we have r(x, θ) instead of p(θ|x).\n",
    "    \"\"\"\n",
    "    print(f\"\\nSampling posterior for x={x_val:.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # MCMC sampling\n",
    "    print(\"[1/2] MCMC (emcee):\")\n",
    "    mcmc_samples = sample_with_emcee(model, x_val,\n",
    "                                     theta_min=theta_min, theta_max=theta_max)\n",
    "\n",
    "    # Nested sampling\n",
    "    print(\"\\n[2/2] Nested Sampling (dynesty):\")\n",
    "    ns_samples = sample_with_dynesty(model, x_val,\n",
    "                                     theta_min=theta_min, theta_max=theta_max)\n",
    "\n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # True posterior (from training data)\n",
    "    x_margin = 0.15\n",
    "    mask = np.abs(x_train - x_val) < x_margin\n",
    "    theta_true = theta_train[mask]\n",
    "\n",
    "    # MCMC\n",
    "    axes[0].hist(mcmc_samples, bins=50, density=True, histtype='step',\n",
    "                label='MCMC', color='C0', lw=4)\n",
    "    axes[0].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[0].set_ylabel('Density', fontsize=16)\n",
    "    axes[0].set_title(f'MCMC: $p(\\\\theta|x={x_val:.2f})$', fontsize=16)\n",
    "    axes[0].legend(fontsize=12)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Nested Sampling\n",
    "    axes[1].hist(ns_samples, bins=50, density=True, histtype='step',\n",
    "                label='Nested Sampling', color='C1', lw=4)\n",
    "    axes[1].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[1].set_ylabel('Density', fontsize=16)\n",
    "    axes[1].set_title(f'Nested Sampling: $p(\\\\theta|x={x_val:.2f})$', fontsize=16)\n",
    "    axes[1].legend(fontsize=12)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    # Overlay comparison\n",
    "    axes[2].hist(mcmc_samples, bins=50, density=True, alpha=0.8,\n",
    "                label='MCMC', color='C0', histtype='step', lw=4)\n",
    "    axes[2].hist(ns_samples, bins=50, density=True, alpha=0.8,\n",
    "                label='Nested Sampling', color='C1', histtype='step', lw=4)\n",
    "    axes[2].hist(theta_true, bins=30, density=True, alpha=0.2,\n",
    "                color='k', label='True', zorder=0)\n",
    "    axes[2].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "    axes[2].set_ylabel('Density', fontsize=16)\n",
    "    axes[2].set_title('Comparison', fontsize=16)\n",
    "    axes[2].legend(fontsize=12)\n",
    "    axes[2].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mcmc_samples, ns_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Gaussian data\n",
    "test_x = 0.5\n",
    "mcmc_gauss, ns_gauss = plot_posterior_comparison(\n",
    "    ratio_estimator_gauss, test_x, theta_joint, x_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Ratio Estimation on Half-Moon Data\n",
    "\n",
    "Now let's see how NRE performs on the half-moon distribution from Tutorials 1 and 2. \n",
    "\n",
    "**Key observation**: Unlike Tutorials 1 and 2 where we needed flows for the half-moon, NRE with a simple classifier can still capture complex distributions because it only needs to **classify** joint vs independent samples, not model the full density!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_halfmoon_joint(n_samples, noise=0.1):\n",
    "    \"\"\"\n",
    "    Sample from half-moon joint distribution p(x, θ).\n",
    "\n",
    "    Same data generator as Tutorials 1 and 2.\n",
    "\n",
    "    Returns:\n",
    "    - theta: parameter values (1D)\n",
    "    - x: observation values (1D)\n",
    "    \"\"\"\n",
    "    samples, labels = make_moons(n_samples=n_samples, noise=noise, random_state=None)\n",
    "    theta = samples[:, 0]  # First dimension\n",
    "    x = samples[:, 1]      # Second dimension\n",
    "    return theta, x\n",
    "\n",
    "# Visualize\n",
    "theta_moon_vis, x_moon_vis = sample_halfmoon_joint(1000)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(theta_moon_vis, x_moon_vis, alpha=0.5, s=20, color='purple')\n",
    "plt.xlabel(r'$\\theta$ (parameter)', fontsize=16)\n",
    "plt.ylabel(r'$x$ (observation)', fontsize=16)\n",
    "plt.title(r'Half-Moon Joint Distribution $p(x, \\theta)$', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Sampled {len(theta_moon_vis)} points from half-moon\")\n",
    "print(f\"θ range: [{theta_moon_vis.min():.2f}, {theta_moon_vis.max():.2f}]\")\n",
    "print(f\"x range: [{x_moon_vis.min():.2f}, {x_moon_vis.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data from half-moon\n",
    "n_training = 10_000\n",
    "print(f\"Generating {n_training} training samples from half-moon...\\n\")\n",
    "\n",
    "# Joint samples\n",
    "theta_joint_moon, x_joint_moon = sample_halfmoon_joint(n_training, noise=0.1)\n",
    "\n",
    "# Independent samples (shuffle to break correlation)\n",
    "theta_indep_moon = theta_joint_moon.copy()\n",
    "x_indep_moon = np.random.permutation(x_joint_moon)\n",
    "\n",
    "print(f\"✓ Training data generated\")\n",
    "print(f\"  Joint samples: {len(theta_joint_moon)}\")\n",
    "print(f\"  Independent samples: {len(theta_indep_moon)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize joint vs independent samples for half-moon\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(theta_joint_moon[:1000], x_joint_moon[:1000], alpha=0.4, s=10, color='C0')\n",
    "axes[0].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[0].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[0].set_title(r'Joint $p(x, \\theta)$ (Positive samples)', fontsize=16)\n",
    "\n",
    "axes[1].scatter(theta_indep_moon[:1000], x_indep_moon[:1000], alpha=0.4, s=10, color='C3')\n",
    "axes[1].set_xlabel(r'$\\theta$', fontsize=16)\n",
    "axes[1].set_ylabel(r'$x$', fontsize=16)\n",
    "axes[1].set_title(r'Independent $p(x)p(\\theta)$ (Negative samples)', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Ratio Estimator on Half-Moon\n",
    "\n",
    "We'll train the same classifier architecture on the half-moon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_estimator_moon = RatioEstimator(hidden_size=64)\n",
    "losses_moon, accs_moon = train_ratio_estimator(\n",
    "    ratio_estimator_moon, theta_joint_moon, x_joint_moon, theta_indep_moon, x_indep_moon, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate: Posterior on Half-Moon with Sampling\n",
    "\n",
    "Let's see how well the ratio estimator captures the complex half-moon posterior using MCMC and nested sampling, **without using flows**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on half-moon data with MCMC and Nested Sampling\n",
    "test_x_moon = 0.5\n",
    "mcmc_moon, ns_moon = plot_posterior_comparison(\n",
    "    ratio_estimator_moon, test_x_moon, theta_joint_moon, x_joint_moon,\n",
    "    theta_min=-1, theta_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision boundary of the classifier\n",
    "print(\"Visualizing classifier decision boundary...\\n\")\n",
    "\n",
    "# Create a grid\n",
    "theta_grid = np.linspace(theta_joint_moon.min(), theta_joint_moon.max(), 100)\n",
    "x_grid = np.linspace(x_joint_moon.min(), x_joint_moon.max(), 100)\n",
    "theta_mesh, x_mesh = np.meshgrid(theta_grid, x_grid)\n",
    "\n",
    "# Evaluate classifier on grid\n",
    "theta_flat = theta_mesh.flatten()\n",
    "x_flat = x_mesh.flatten()\n",
    "theta_tensor = torch.tensor(theta_flat, dtype=torch.float32).reshape(-1, 1)\n",
    "x_tensor = torch.tensor(x_flat, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    probs = ratio_estimator_moon(x_tensor, theta_tensor).squeeze().numpy()\n",
    "\n",
    "probs_mesh = probs.reshape(theta_mesh.shape)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot decision boundary\n",
    "contour = ax.contourf(theta_mesh, x_mesh, probs_mesh, levels=10, cmap='RdBu_r', alpha=0.8)\n",
    "ax.contour(theta_mesh, x_mesh, probs_mesh, levels=[0.5], colors='k', linewidths=2)\n",
    "\n",
    "# Overlay training data\n",
    "ax.scatter(theta_joint_moon[:500], x_joint_moon[:500], alpha=0.5, s=10,\n",
    "           label='Joint samples', color='blue', edgecolor='k', linewidth=0.5)\n",
    "ax.scatter(theta_indep_moon[:500], x_indep_moon[:500], alpha=0.5, s=10,\n",
    "           label='Independent samples', color='red', edgecolor='k', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(r'Parameter $\\theta$', fontsize=16)\n",
    "ax.set_ylabel(r'Data $x$', fontsize=16)\n",
    "ax.set_title('Ratio Estimator Decision Boundary', fontsize=16)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "cbar.set_label(r'$d(x, \\theta)$ [P(joint | x, θ)]', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we explored Neural Ratio Estimation (NRE) and demonstrated its effectiveness on both Gaussian and non-Gaussian data.\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "**Part 1: Ratio Estimation on Gaussian Data**\n",
    "- Trained a binary classifier to distinguish joint samples $p(x, \\theta)$ from independent samples $p(x)p(\\theta)$\n",
    "- Used the classifier to compute the likelihood ratio $r(x, \\theta)$\n",
    "- Used MCMC and Nested Sampling to obtain posterior samples\n",
    "- Achieved accurate posterior estimates\n",
    "\n",
    "**Part 2: Ratio Estimation on Half-Moon Data**\n",
    "- Applied the same classifier to non-Gaussian data\n",
    "- **Successfully captured complex posterior structure without flows!**\n",
    "- Both MCMC and Nested Sampling produced accurate posteriors\n",
    "\n",
    "### Comparison with Previous Tutorials\n",
    "\n",
    "| Aspect | NPE (Tutorial 1) | NLE (Tutorial 2) | NRE (Tutorial 3) |\n",
    "|--------|------------------|------------------|------------------|\n",
    "| **What we learn** | $p(\\theta\\|x)$ | $p(x\\|\\theta)$ | $r(x,\\theta) = \\frac{p(x,\\theta)}{p(x)p(\\theta)}$ |\n",
    "| **Model type** | Density model | Density model | Binary classifier |\n",
    "| **Inference** | Direct sampling | MCMC/Nested Sampling | MCMC/Nested Sampling |\n",
    "| **Flexibility** | Needs flows for complex posteriors | Needs flows for complex likelihoods | Simple classifier works! |\n",
    "| **Training data** | $(x, \\theta)$ pairs | $(x, \\theta)$ pairs | Joint + Independent pairs |\n",
    "| **Advantage** | Fast direct sampling | Flexible sampling | No density modeling needed |\n",
    "\n",
    "### Key Advantages of NRE\n",
    "\n",
    "1. **No density modeling**: Only needs to classify, not model full distributions\n",
    "2. **Simple architecture**: Works well without flows, even for complex distributions\n",
    "3. **Sample efficient**: Often needs fewer training samples than NPE/NLE\n",
    "4. **Interpretable**: The classifier directly learns what distinguishes joint from independent samples\n",
    "\n",
    "### Key Disadvantages of NRE\n",
    "\n",
    "1. **Requires negative samples**: Must generate samples from $p(x)p(\\theta)$\n",
    "2. **Requires sampling**: Like NLE, needs MCMC or nested sampling for inference (except for very low-dimensional $\\theta$ where grid evaluation is feasible)\n",
    "3. **Ratio instability**: Can be numerically unstable when classifier is very confident\n",
    "\n",
    "### When to Use NRE?\n",
    "\n",
    "- **Use NRE** when:\n",
    "  - You want a simple model without flows\n",
    "  - You can easily generate independent samples\n",
    "  - You're comfortable with MCMC/Nested Sampling\n",
    "  - Sample efficiency is important\n",
    "\n",
    "- **Use NPE (Tutorial 1)** when:\n",
    "  - You want fast inference via direct sampling\n",
    "  - You have high-dimensional parameters\n",
    "  - You're willing to use flow models\n",
    "\n",
    "- **Use NLE (Tutorial 2)** when:\n",
    "  - Likelihood modeling is more natural for your problem\n",
    "  - You need evidence estimates\n",
    "  - You want maximum flexibility in choosing sampling algorithms\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**NRE is a powerful alternative that trades density modeling for classification.** This makes it remarkably effective at capturing complex distributions without requiring normalizing flows. The main advantage over NPE and NLE is that the classifier can be much simpler (no flows needed), while the main similarity with NLE is that both require sampling for inference.\n",
    "\n",
    "**For problems where sampling is acceptable and you want the simplest possible model, NRE is an excellent choice!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
